{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 05 - Patient Risk Clustering\n## Discovering Natural Patient Risk Groups for DDI Analysis\n\n---\n\n**Objective:** Use unsupervised machine learning (clustering) to discover natural groupings of patients based on their DDI risk profiles.\n\n**Approach:**\n- K-means clustering on patient-level features\n- Determine optimal number of clusters using elbow method and silhouette analysis\n- Characterize and name clusters based on clinical characteristics\n- Validate cluster quality statistically and clinically\n- Save clustered patient data for downstream analysis\n\n**Expected Outcomes:**\n- 3-5 distinct patient risk clusters (e.g., \"Elderly High-Risk Polypharmacy\", \"Young Low-Risk\")\n- Patient features enriched with cluster labels\n- Visualizations showing cluster separation\n- Foundation for targeted clinical interventions\n\n**References:**\n- See `../docs/CLUSTERING_AND_ANALYSIS_GUIDE.md` for detailed methodology\n- See `../docs/FEATURE_ENGINEERING_GUIDE.md` for feature descriptions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Metrics and validation\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    silhouette_samples\n",
    ")\n",
    "\n",
    "# Statistical testing\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# S3/MinIO access\n",
    "import s3fs\n",
    "import boto3\n",
    "\n",
    "# Configuration\n",
    "from config import *\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "logger.info(\"Setup complete - imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Patient Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 filesystem\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=MINIO_ACCESS_KEY,\n",
    "    secret=MINIO_SECRET_KEY,\n",
    "    client_kwargs={'endpoint_url': f'http://{MINIO_ENDPOINT}'}\n",
    ")\n",
    "\n",
    "# Load patient features from v3_features tier\n",
    "input_uri = f\"s3://{DEST_BUCKET}/v3_features/ddi/patients_features.parquet\"\n",
    "logger.info(f\"Loading patient features from: {input_uri}\")\n",
    "\n",
    "try:\n",
    "    patient_features = pd.read_parquet(input_uri, filesystem=fs)\n",
    "    logger.info(f\"Loaded {len(patient_features)} patient records\")\n",
    "    logger.info(f\"Features available: {len(patient_features.columns)} columns\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading patient features: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\n=== Patient Features Dataset ===\")\n",
    "print(f\"Number of patients: {len(patient_features)}\")\n",
    "print(f\"Number of features: {len(patient_features.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "patient_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine feature data types and completeness\n",
    "print(\"\\n=== Feature Info ===\")\n",
    "patient_features.info()\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing = patient_features.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Selection for Clustering\n",
    "\n",
    "**Important:** Not all features should be used for clustering. We select features based on:\n",
    "- **Clinical relevance** - Features that matter for risk stratification\n",
    "- **Variability** - Features that differ across patients\n",
    "- **Independence** - Avoid highly correlated features\n",
    "\n",
    "We'll use features that capture:\n",
    "1. Demographics (Age, Gender)\n",
    "2. Medication profile (unique medications, diversity, temporal patterns)\n",
    "3. DDI risk (counts, severity, density)\n",
    "4. Care complexity (source diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clustering features\n",
    "# Excludes: PatientSID (ID), dates (temporal), derived binary flags that are combinations\n",
    "clustering_features = [\n",
    "    # Demographics\n",
    "    'Age',\n",
    "    'Gender',  # Will need encoding\n",
    "\n",
    "    # Medication profile\n",
    "    'unique_medications',\n",
    "    'medication_diversity',\n",
    "    'avg_medications_per_day',\n",
    "\n",
    "    # Temporal\n",
    "    'medication_timespan_days',\n",
    "\n",
    "    # Source system\n",
    "    'source_diversity',\n",
    "\n",
    "    # DDI risk metrics\n",
    "    'ddi_pair_count',\n",
    "    'ddi_severity_Moderate',  # Changed: Only Moderate severity exists\n",
    "    'ddi_severity_Low',       # Added: Low severity is available\n",
    "    'total_ddi_risk_score',\n",
    "    'ddi_density',\n",
    "    'max_severity_level'\n",
    "]\n",
    "\n",
    "logger.info(f\"Selected {len(clustering_features)} features for clustering\")\n",
    "print(\"\\n=== Clustering Features ===\")\n",
    "for i, feat in enumerate(clustering_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in clustering_features if f not in patient_features.columns]\n",
    "if missing_features:\n",
    "    logger.error(f\"Missing features: {missing_features}\")\n",
    "    raise ValueError(f\"Features not found in dataset: {missing_features}\")\n",
    "else:\n",
    "    logger.info(\"All clustering features present in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Preprocessing\n",
    "\n",
    "### Why Preprocessing Matters:\n",
    "1. **Categorical encoding**: Convert Gender to numeric (M=1, F=0, U=-1)\n",
    "2. **Missing values**: Handle any NaN values\n",
    "3. **Scaling**: Standardize features so each contributes equally to distance calculations\n",
    "\n",
    "Without scaling, features with larger numeric ranges would dominate clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering features\n",
    "X = patient_features[clustering_features].copy()\n",
    "\n",
    "print(\"=== Pre-processing Steps ===\")\n",
    "print(f\"\\nOriginal shape: {X.shape}\")\n",
    "\n",
    "# Step 1: Encode Gender (categorical variable)\n",
    "print(\"\\n1. Encoding Gender...\")\n",
    "print(f\"   Gender distribution before encoding:\")\n",
    "print(f\"   {X['Gender'].value_counts()}\")\n",
    "\n",
    "gender_mapping = {'M': 1, 'F': 0, 'U': -1}\n",
    "X['Gender'] = X['Gender'].map(gender_mapping)\n",
    "print(f\"   Encoding: M=1, F=0, U=-1\")\n",
    "\n",
    "# Step 2: Check for missing values\n",
    "print(\"\\n2. Checking for missing values...\")\n",
    "missing_counts = X.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(\"   Missing values found:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    \n",
    "    # Simple imputation: use median for numeric features\n",
    "    print(\"   Imputing with median values...\")\n",
    "    X = X.fillna(X.median())\n",
    "    logger.info(\"Missing values imputed with median\")\n",
    "else:\n",
    "    print(\"   No missing values detected\")\n",
    "\n",
    "# Step 3: Scale features\n",
    "print(\"\\n3. Scaling features (StandardScaler)...\")\n",
    "print(\"   Before scaling - feature ranges:\")\n",
    "print(X.describe().loc[['min', 'max']].T)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "print(\"\\n   After scaling - all features have mean‚âà0, std‚âà1:\")\n",
    "print(X_scaled_df.describe().loc[['mean', 'std']].T)\n",
    "\n",
    "logger.info(f\"Preprocessing complete - final shape: {X_scaled.shape}\")\n",
    "print(f\"\\nFinal preprocessing result: {X_scaled.shape[0]} samples, {X_scaled.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Determine Optimal Number of Clusters (K)\n",
    "\n",
    "**Challenge:** K-means requires specifying the number of clusters upfront.\n",
    "\n",
    "**Solution:** Use two methods to find optimal K:\n",
    "1. **Elbow Method** - Look for \"elbow\" in inertia plot\n",
    "2. **Silhouette Score** - Find K with best cluster separation\n",
    "\n",
    "We'll test K from 2 to 10 and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test range of K values\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "logger.info(f\"Testing K from {min(K_range)} to {max(K_range)}...\")\n",
    "\n",
    "for k in K_range:\n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, labels))\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    \n",
    "    logger.info(f\"K={k}: inertia={kmeans.inertia_:.2f}, silhouette={silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "logger.info(\"Cluster evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster quality metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Elbow Method - Inertia\n",
    "axes[0, 0].plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "axes[0, 0].set_title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(K_range)\n",
    "\n",
    "# 2. Silhouette Score (Higher is better)\n",
    "axes[0, 1].plot(K_range, silhouette_scores, marker='o', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[0, 1].set_title('Silhouette Score vs K (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(K_range)\n",
    "axes[0, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Good separation (>0.5)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Mark best silhouette score\n",
    "best_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette = max(silhouette_scores)\n",
    "axes[0, 1].annotate(\n",
    "    f'Best K={best_k_silhouette}\\n({best_silhouette:.3f})',\n",
    "    xy=(best_k_silhouette, best_silhouette),\n",
    "    xytext=(best_k_silhouette + 1, best_silhouette - 0.05),\n",
    "    arrowprops=dict(arrowstyle='->', color='red'),\n",
    "    fontsize=10,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "# 3. Davies-Bouldin Index (Lower is better)\n",
    "axes[1, 0].plot(K_range, davies_bouldin_scores, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index', fontsize=12)\n",
    "axes[1, 0].set_title('Davies-Bouldin Index vs K (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(K_range)\n",
    "\n",
    "# 4. Calinski-Harabasz Index (Higher is better)\n",
    "axes[1, 1].plot(K_range, calinski_harabasz_scores, marker='o', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Index', fontsize=12)\n",
    "axes[1, 1].set_title('Calinski-Harabasz Index vs K (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(K_range)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print recommendations\n",
    "print(\"\\n=== Cluster Quality Metrics Summary ===\")\n",
    "print(f\"\\nBest K by Silhouette Score: {best_k_silhouette} (score: {best_silhouette:.3f})\")\n",
    "print(f\"Best K by Davies-Bouldin: {K_range[np.argmin(davies_bouldin_scores)]} (score: {min(davies_bouldin_scores):.3f})\")\n",
    "print(f\"Best K by Calinski-Harabasz: {K_range[np.argmax(calinski_harabasz_scores)]} (score: {max(calinski_harabasz_scores):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal K based on analysis\n",
    "# Prioritize silhouette score with clinical sensibility check\n",
    "optimal_k = best_k_silhouette\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SELECTED OPTIMAL K = {optimal_k}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nRationale:\")\n",
    "print(f\"  - Silhouette score: {silhouette_scores[optimal_k-2]:.3f} (good separation)\")\n",
    "print(f\"  - Clinically interpretable: {optimal_k} patient groups\")\n",
    "print(f\"  - Actionable: Can define distinct interventions for each group\")\n",
    "\n",
    "logger.info(f\"Optimal K selected: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Apply K-Means Clustering\n",
    "\n",
    "Now we'll apply K-means with the optimal K and assign cluster labels to all patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final K-means model\n",
    "logger.info(f\"Fitting K-means with K={optimal_k}...\")\n",
    "\n",
    "kmeans_final = KMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original patient features\n",
    "patient_features['cluster'] = cluster_labels\n",
    "\n",
    "# Calculate final metrics\n",
    "final_silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "final_inertia = kmeans_final.inertia_\n",
    "\n",
    "logger.info(f\"Clustering complete - Silhouette: {final_silhouette:.3f}, Inertia: {final_inertia:.2f}\")\n",
    "\n",
    "print(f\"\\n=== Clustering Results ===\")\n",
    "print(f\"Number of clusters: {optimal_k}\")\n",
    "print(f\"Silhouette score: {final_silhouette:.3f}\")\n",
    "print(f\"Inertia: {final_inertia:.2f}\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "print(patient_features['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cluster Characterization\n",
    "\n",
    "For each cluster, we'll calculate:\n",
    "- Mean and median of key features\n",
    "- Patient count and percentage\n",
    "- Clinical indicators (elderly, polypharmacy, high risk)\n",
    "\n",
    "This helps us understand what makes each cluster unique and clinically meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key features for characterization\n",
    "characterization_features = [\n",
    "    'Age',\n",
    "    'unique_medications',\n",
    "    'ddi_pair_count',\n",
    "    'total_ddi_risk_score',\n",
    "    'ddi_density',\n",
    "    'medication_timespan_days',\n",
    "    'source_diversity',\n",
    "    'max_severity_level'\n",
    "]\n",
    "\n",
    "# Calculate cluster statistics\n",
    "cluster_summary = patient_features.groupby('cluster').agg({\n",
    "    'PatientSID': 'count',  # Number of patients\n",
    "    'Age': ['mean', 'median'],\n",
    "    'unique_medications': ['mean', 'median'],\n",
    "    'ddi_pair_count': ['mean', 'median'],\n",
    "    'total_ddi_risk_score': ['mean', 'median'],\n",
    "    'ddi_density': ['mean', 'median'],\n",
    "    'is_polypharmacy': 'sum',  # Count of polypharmacy patients\n",
    "    'is_high_ddi_risk': 'sum',  # Count of high-risk patients\n",
    "    'IsElderly': 'sum'  # Count of elderly patients\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "cluster_summary.columns = ['_'.join(col).strip() if col[1] else col[0] for col in cluster_summary.columns.values]\n",
    "cluster_summary = cluster_summary.rename(columns={'PatientSID_count': 'patient_count'})\n",
    "\n",
    "# Calculate percentages\n",
    "total_patients = len(patient_features)\n",
    "cluster_summary['percentage'] = (cluster_summary['patient_count'] / total_patients * 100).round(1)\n",
    "cluster_summary['polypharmacy_rate'] = (cluster_summary['is_polypharmacy_sum'] / cluster_summary['patient_count'] * 100).round(1)\n",
    "cluster_summary['high_risk_rate'] = (cluster_summary['is_high_ddi_risk_sum'] / cluster_summary['patient_count'] * 100).round(1)\n",
    "cluster_summary['elderly_rate'] = (cluster_summary['IsElderly_sum'] / cluster_summary['patient_count'] * 100).round(1)\n",
    "\n",
    "print(\"\\n=== Cluster Characterization Summary ===\")\n",
    "print(cluster_summary[[\n",
    "    'patient_count', 'percentage',\n",
    "    'Age_mean', 'unique_medications_mean', 'ddi_pair_count_mean',\n",
    "    'total_ddi_risk_score_mean', 'elderly_rate', 'polypharmacy_rate', 'high_risk_rate'\n",
    "]].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name clusters based on characteristics\n",
    "# This is done manually based on the cluster summary\n",
    "# You'll need to review the summary above and assign meaningful names\n",
    "\n",
    "# Example cluster naming (adjust based on actual results)\n",
    "cluster_names = {}\n",
    "\n",
    "# Analyze each cluster to assign names\n",
    "for cluster_id in range(optimal_k):\n",
    "    row = cluster_summary.loc[cluster_id]\n",
    "    \n",
    "    # Determine characteristics\n",
    "    avg_age = row['Age_mean']\n",
    "    avg_meds = row['unique_medications_mean']\n",
    "    avg_ddi = row['ddi_pair_count_mean']\n",
    "    elderly_pct = row['elderly_rate']\n",
    "    polypharm_pct = row['polypharmacy_rate']\n",
    "    high_risk_pct = row['high_risk_rate']\n",
    "    \n",
    "    # Name based on characteristics\n",
    "    if elderly_pct > 70 and polypharm_pct > 70 and high_risk_pct > 70:\n",
    "        name = \"Elderly High-Risk Polypharmacy\"\n",
    "    elif avg_age < 50 and avg_ddi < 2 and high_risk_pct < 30:\n",
    "        name = \"Young Low-Risk\"\n",
    "    elif avg_age >= 50 and avg_age < 65 and polypharm_pct > 40:\n",
    "        name = \"Middle-Age Moderate Complexity\"\n",
    "    elif avg_ddi > 3 and high_risk_pct > 60:\n",
    "        name = \"High DDI Burden\"\n",
    "    else:\n",
    "        name = f\"Cluster {cluster_id}\"\n",
    "    \n",
    "    cluster_names[cluster_id] = name\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}: {name}\")\n",
    "    print(f\"  - Age: {avg_age:.1f} years ({elderly_pct:.0f}% elderly)\")\n",
    "    print(f\"  - Medications: {avg_meds:.1f} unique ({polypharm_pct:.0f}% polypharmacy)\")\n",
    "    print(f\"  - DDI pairs: {avg_ddi:.1f} ({high_risk_pct:.0f}% high risk)\")\n",
    "    print(f\"  - Patients: {int(row['patient_count'])} ({row['percentage']:.1f}%)\")\n",
    "\n",
    "# Add cluster names to patient features\n",
    "patient_features['cluster_name'] = patient_features['cluster'].map(cluster_names)\n",
    "\n",
    "logger.info(f\"Clusters named: {list(cluster_names.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Cluster Visualization\n",
    "\n",
    "We'll use PCA (Principal Component Analysis) to reduce our 14-dimensional feature space to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create PCA scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each cluster with different color\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0],\n",
    "    X_pca[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Plot cluster centroids\n",
    "centroids_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "plt.scatter(\n",
    "    centroids_pca[:, 0],\n",
    "    centroids_pca[:, 1],\n",
    "    c='red',\n",
    "    s=300,\n",
    "    alpha=0.8,\n",
    "    marker='X',\n",
    "    edgecolors='black',\n",
    "    linewidth=2,\n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('Patient Clusters (PCA 2D Projection)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA Variance Explained: {pca.explained_variance_ratio_.sum():.1%} total\")\n",
    "print(f\"  PC1: {pca.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"  PC2: {pca.explained_variance_ratio_[1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of cluster characteristics\n",
    "# Select key features for heatmap\n",
    "heatmap_features = [\n",
    "    'Age_mean',\n",
    "    'unique_medications_mean',\n",
    "    'ddi_pair_count_mean',\n",
    "    'total_ddi_risk_score_mean',\n",
    "    'ddi_density_mean',\n",
    "    'elderly_rate',\n",
    "    'polypharmacy_rate',\n",
    "    'high_risk_rate'\n",
    "]\n",
    "\n",
    "# Create heatmap data\n",
    "heatmap_data = cluster_summary[heatmap_features].T\n",
    "heatmap_data.columns = [cluster_names[i] for i in heatmap_data.columns]\n",
    "\n",
    "# Normalize for better visualization (0-1 scale)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_hm = MinMaxScaler()\n",
    "heatmap_normalized = pd.DataFrame(\n",
    "    scaler_hm.fit_transform(heatmap_data),\n",
    "    columns=heatmap_data.columns,\n",
    "    index=heatmap_data.index\n",
    ")\n",
    "\n",
    "# Rename index for readability\n",
    "index_labels = [\n",
    "    'Mean Age',\n",
    "    'Mean Medications',\n",
    "    'Mean DDI Pairs',\n",
    "    'Mean Risk Score',\n",
    "    'Mean DDI Density',\n",
    "    'Elderly Rate (%)',\n",
    "    'Polypharmacy Rate (%)',\n",
    "    'High Risk Rate (%)'\n",
    "]\n",
    "heatmap_normalized.index = index_labels\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_normalized,\n",
    "    annot=heatmap_data.round(1),\n",
    "    fmt='.1f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Normalized Value (0-1)'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "plt.title('Cluster Characteristics Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts comparing clusters on key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Prepare data with cluster names\n",
    "plot_data = cluster_summary.copy()\n",
    "plot_data['cluster_name'] = [cluster_names[i] for i in plot_data.index]\n",
    "\n",
    "# 1. Age comparison\n",
    "plot_data.plot(x='cluster_name', y='Age_mean', kind='bar', ax=axes[0, 0], color='steelblue', legend=False)\n",
    "axes[0, 0].set_title('Mean Age by Cluster', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Cluster', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Mean Age (years)', fontsize=10)\n",
    "axes[0, 0].set_xticklabels(plot_data['cluster_name'], rotation=45, ha='right')\n",
    "\n",
    "# 2. Medication count comparison\n",
    "plot_data.plot(x='cluster_name', y='unique_medications_mean', kind='bar', ax=axes[0, 1], color='green', legend=False)\n",
    "axes[0, 1].set_title('Mean Unique Medications by Cluster', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Cluster', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Mean Unique Medications', fontsize=10)\n",
    "axes[0, 1].set_xticklabels(plot_data['cluster_name'], rotation=45, ha='right')\n",
    "\n",
    "# 3. DDI risk score comparison\n",
    "plot_data.plot(x='cluster_name', y='total_ddi_risk_score_mean', kind='bar', ax=axes[1, 0], color='orange', legend=False)\n",
    "axes[1, 0].set_title('Mean DDI Risk Score by Cluster', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Cluster', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Mean Risk Score', fontsize=10)\n",
    "axes[1, 0].set_xticklabels(plot_data['cluster_name'], rotation=45, ha='right')\n",
    "\n",
    "# 4. High risk rate comparison\n",
    "plot_data.plot(x='cluster_name', y='high_risk_rate', kind='bar', ax=axes[1, 1], color='red', legend=False)\n",
    "axes[1, 1].set_title('High DDI Risk Rate by Cluster', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Cluster', fontsize=10)\n",
    "axes[1, 1].set_ylabel('High Risk Rate (%)', fontsize=10)\n",
    "axes[1, 1].set_xticklabels(plot_data['cluster_name'], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Statistical Validation\n",
    "\n",
    "We'll use ANOVA to test if clusters have significantly different mean DDI risk scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA to test if clusters differ significantly\n",
    "cluster_groups = [\n",
    "    patient_features[patient_features['cluster'] == i]['total_ddi_risk_score'].values\n",
    "    for i in range(optimal_k)\n",
    "]\n",
    "\n",
    "f_stat, p_value = f_oneway(*cluster_groups)\n",
    "\n",
    "print(\"\\n=== Statistical Validation (ANOVA) ===\")\n",
    "print(f\"Null Hypothesis: All clusters have the same mean DDI risk score\")\n",
    "print(f\"\\nF-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ RESULT: Clusters have significantly different mean DDI risk scores (p < 0.05)\")\n",
    "    print(f\"   Conclusion: The clustering successfully identified distinct patient risk groups\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RESULT: No significant difference between clusters (p >= 0.05)\")\n",
    "    print(f\"   Conclusion: Consider different K or feature selection\")\n",
    "\n",
    "logger.info(f\"ANOVA: F={f_stat:.4f}, p={p_value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-sample silhouette scores\n",
    "silhouette_vals = silhouette_samples(X_scaled, cluster_labels)\n",
    "patient_features['silhouette_score'] = silhouette_vals\n",
    "\n",
    "# Analyze silhouette scores by cluster\n",
    "print(\"\\n=== Silhouette Scores by Cluster ===\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_silhouette_vals = silhouette_vals[cluster_labels == cluster_id]\n",
    "    print(f\"\\n{cluster_names[cluster_id]}:\")\n",
    "    print(f\"  Mean silhouette: {cluster_silhouette_vals.mean():.3f}\")\n",
    "    print(f\"  Min silhouette: {cluster_silhouette_vals.min():.3f}\")\n",
    "    print(f\"  Max silhouette: {cluster_silhouette_vals.max():.3f}\")\n",
    "    \n",
    "    # Count poorly assigned patients (negative silhouette)\n",
    "    poorly_assigned = (cluster_silhouette_vals < 0).sum()\n",
    "    if poorly_assigned > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  {poorly_assigned} patients may be misclassified (silhouette < 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Save Clustered Data\n",
    "\n",
    "Save the patient features enriched with cluster assignments for use in 06_analysis.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustered patient data\n",
    "output_uri = f\"s3://{DEST_BUCKET}/v3_features/ddi/patients_features_clustered.parquet\"\n",
    "logger.info(f\"Saving clustered patient data to: {output_uri}\")\n",
    "\n",
    "try:\n",
    "    patient_features.to_parquet(output_uri, filesystem=fs, index=False)\n",
    "    logger.info(f\"Successfully saved {len(patient_features)} patient records with cluster labels\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving clustered data: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n‚úÖ Clustered patient data saved to: {output_uri}\")\n",
    "print(f\"   Records: {len(patient_features)}\")\n",
    "print(f\"   Columns: {len(patient_features.columns)}\")\n",
    "print(f\"   New columns: cluster, cluster_name, silhouette_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster summary statistics\n",
    "cluster_summary_path = \"cluster_summary.csv\"\n",
    "cluster_summary.to_csv(cluster_summary_path)\n",
    "logger.info(f\"Cluster summary saved to: {cluster_summary_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Cluster summary saved to: {cluster_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering metrics\n",
    "import json\n",
    "\n",
    "metrics = {\n",
    "    'optimal_k': int(optimal_k),\n",
    "    'silhouette_score': float(final_silhouette),\n",
    "    'inertia': float(final_inertia),\n",
    "    'davies_bouldin_score': float(davies_bouldin_score(X_scaled, cluster_labels)),\n",
    "    'calinski_harabasz_score': float(calinski_harabasz_score(X_scaled, cluster_labels)),\n",
    "    'anova_f_statistic': float(f_stat),\n",
    "    'anova_p_value': float(p_value),\n",
    "    'pca_variance_explained': float(pca.explained_variance_ratio_.sum()),\n",
    "    'cluster_names': cluster_names,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metrics_path = \"cluster_metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "logger.info(f\"Clustering metrics saved to: {metrics_path}\")\n",
    "print(f\"\\n‚úÖ Clustering metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Summary and Clinical Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*20 + \"CLUSTERING ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä CLUSTERING RESULTS:\")\n",
    "print(f\"   ‚Ä¢ Number of clusters identified: {optimal_k}\")\n",
    "print(f\"   ‚Ä¢ Total patients clustered: {len(patient_features)}\")\n",
    "print(f\"   ‚Ä¢ Silhouette score: {final_silhouette:.3f} (>0.5 = good separation)\")\n",
    "print(f\"   ‚Ä¢ Statistical validation: {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'} (p={p_value:.6f})\")\n",
    "\n",
    "print(f\"\\nüë• IDENTIFIED PATIENT GROUPS:\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    name = cluster_names[cluster_id]\n",
    "    count = cluster_summary.loc[cluster_id, 'patient_count']\n",
    "    pct = cluster_summary.loc[cluster_id, 'percentage']\n",
    "    avg_age = cluster_summary.loc[cluster_id, 'Age_mean']\n",
    "    avg_meds = cluster_summary.loc[cluster_id, 'unique_medications_mean']\n",
    "    avg_risk = cluster_summary.loc[cluster_id, 'total_ddi_risk_score_mean']\n",
    "    \n",
    "    print(f\"\\n   {cluster_id+1}. {name}\")\n",
    "    print(f\"      Patients: {int(count)} ({pct:.1f}%)\")\n",
    "    print(f\"      Avg Age: {avg_age:.1f} years\")\n",
    "    print(f\"      Avg Medications: {avg_meds:.1f}\")\n",
    "    print(f\"      Avg DDI Risk Score: {avg_risk:.1f}\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUTS CREATED:\")\n",
    "print(f\"   ‚Ä¢ {output_uri}\")\n",
    "print(f\"   ‚Ä¢ {cluster_summary_path}\")\n",
    "print(f\"   ‚Ä¢ {metrics_path}\")\n",
    "\n",
    "print(f\"\\nüìà NEXT STEPS:\")\n",
    "print(f\"   1. Review cluster names and adjust if needed\")\n",
    "print(f\"   2. Proceed to 06_analysis.ipynb for deep-dive analysis\")\n",
    "print(f\"   3. Use cluster labels to develop targeted interventions\")\n",
    "print(f\"   4. Consider cluster-specific predictive models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Clustering notebook execution complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}