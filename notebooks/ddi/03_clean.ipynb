{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Cleaning - DDI, Medications, and Demographics\n",
    "This notebook cleans and preprocesses the raw data from v1_raw and writes cleaned versions to v2_clean.\n",
    "\n",
    "**Input**:  \n",
    "- `med-data/v1_raw/ddi/db_drug_interactions.parquet`  \n",
    "- `med-data/v1_raw/medications/medications_combined.parquet`  \n",
    "- `med-data/v1_raw/demographics/patient_demographics.parquet`\n",
    "\n",
    "**Output**:  \n",
    "- `med-data/v2_clean/ddi/db_drug_interactions_clean.parquet`  \n",
    "- `med-data/v2_clean/medications/medications_clean.parquet`  \n",
    "- `med-data/v2_clean/demographics/patient_demographics_clean.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from importlib.metadata import version\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "version-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dependencies\n",
    "\n",
    "def print_version():\n",
    "    print(\"pandas:\", pd.__version__)\n",
    "    print(\"numpy:\", np.__version__)\n",
    "    print(\"s3fs:\", s3fs.__version__)\n",
    "    print(\"pyarrow:\", pa.__version__)\n",
    "\n",
    "print_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logging-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "logging.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "\n",
    "logging.info(f\"MinIO endpoint: {MINIO_ENDPOINT}\")\n",
    "logging.info(f\"Source: {DEST_BUCKET}/v1_raw/\")\n",
    "logging.info(f\"Destination: {DEST_BUCKET}/v2_clean/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3fs-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3FileSystem for MinIO\n",
    "\n",
    "logging.info(f\"Initializing S3FileSystem for MinIO at {MINIO_ENDPOINT}\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    key=MINIO_ACCESS_KEY,\n",
    "    secret=MINIO_SECRET_KEY,\n",
    "    client_kwargs={'endpoint_url': f\"http://{MINIO_ENDPOINT}\"}\n",
    ")\n",
    "logging.info(\"S3FileSystem created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-ddi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DDI reference dataset from v1_raw\n",
    "\n",
    "ddi_uri = f\"s3://{DEST_BUCKET}/{V1_RAW_DDI_PREFIX}db_drug_interactions.parquet\"\n",
    "logging.info(f\"Reading DDI data: {ddi_uri}\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_ddi_raw = pd.read_parquet(ddi_uri, filesystem=fs)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "logging.info(f\"Loaded {len(df_ddi_raw):,} DDI records in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nDDI Raw Data Shape: {df_ddi_raw.shape}\")\n",
    "df_ddi_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-medications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load medications dataset from v1_raw\n",
    "\n",
    "meds_uri = f\"s3://{DEST_BUCKET}/{V1_RAW_MEDICATIONS_PREFIX}medications_combined.parquet\"\n",
    "logging.info(f\"Reading medications data: {meds_uri}\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_meds_raw = pd.read_parquet(meds_uri, filesystem=fs)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "logging.info(f\"Loaded {len(df_meds_raw):,} medication records in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nMedications Raw Data Shape: {df_meds_raw.shape}\")\n",
    "df_meds_raw.head()\n",
    "\n",
    "# Load demographics dataset from v1_raw\n",
    "\n",
    "demo_uri = f\"s3://{DEST_BUCKET}/v1_raw/demographics/patient_demographics.parquet\"\n",
    "logging.info(f\"Reading demographics data: {demo_uri}\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_demo_raw = pd.read_parquet(demo_uri, filesystem=fs)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "logging.info(f\"Loaded {len(df_demo_raw):,} patient demographics records in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nDemographics Raw Data Shape: {df_demo_raw.shape}\")\n",
    "df_demo_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Clean DDI Reference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddi-initial-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data quality assessment for DDI dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DDI DATASET - INITIAL QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nShape: {df_ddi_raw.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_ddi_raw.isnull().sum())\n",
    "\n",
    "print(f\"\\nDuplicate rows: {df_ddi_raw.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df_ddi_raw.dtypes)\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddi-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean DDI dataset\n",
    "\n",
    "logging.info(\"Cleaning DDI dataset...\")\n",
    "\n",
    "# Create copy for cleaning\n",
    "df_ddi_clean = df_ddi_raw.copy()\n",
    "\n",
    "initial_count = len(df_ddi_clean)\n",
    "logging.info(f\"Starting with {initial_count:,} records\")\n",
    "\n",
    "# 1. Remove duplicates\n",
    "df_ddi_clean = df_ddi_clean.drop_duplicates()\n",
    "duplicates_removed = initial_count - len(df_ddi_clean)\n",
    "logging.info(f\"Removed {duplicates_removed:,} duplicate records\")\n",
    "\n",
    "# 2. Remove records with missing values\n",
    "before_nulls = len(df_ddi_clean)\n",
    "df_ddi_clean = df_ddi_clean.dropna()\n",
    "nulls_removed = before_nulls - len(df_ddi_clean)\n",
    "logging.info(f\"Removed {nulls_removed:,} records with missing values\")\n",
    "\n",
    "# 3. Strip whitespace from string columns\n",
    "df_ddi_clean['Drug 1'] = df_ddi_clean['Drug 1'].str.strip()\n",
    "df_ddi_clean['Drug 2'] = df_ddi_clean['Drug 2'].str.strip()\n",
    "df_ddi_clean['Interaction Description'] = df_ddi_clean['Interaction Description'].str.strip()\n",
    "logging.info(\"Stripped whitespace from string columns\")\n",
    "\n",
    "# 4. Remove empty strings\n",
    "before_empty = len(df_ddi_clean)\n",
    "df_ddi_clean = df_ddi_clean[\n",
    "    (df_ddi_clean['Drug 1'] != '') & \n",
    "    (df_ddi_clean['Drug 2'] != '') & \n",
    "    (df_ddi_clean['Interaction Description'] != '')\n",
    "]\n",
    "empty_removed = before_empty - len(df_ddi_clean)\n",
    "logging.info(f\"Removed {empty_removed:,} records with empty strings\")\n",
    "\n",
    "# 5. Add normalized drug name columns (for matching)\n",
    "def normalize_drug_name(drug_name):\n",
    "    \"\"\"Normalize drug name: uppercase, remove salt suffixes.\"\"\"\n",
    "    if pd.isna(drug_name):\n",
    "        return None\n",
    "    name = str(drug_name).upper().strip()\n",
    "    suffixes = [' HCL', ' HYDROCHLORIDE', ' SODIUM', ' POTASSIUM', ' CALCIUM',\n",
    "                ' SULFATE', ' TARTRATE', ' SUCCINATE', ' MALEATE', ' FUMARATE',\n",
    "                ' ACETATE', ' CITRATE', ' PHOSPHATE']\n",
    "    for suffix in suffixes:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[:-len(suffix)].strip()\n",
    "            break\n",
    "    return name\n",
    "\n",
    "df_ddi_clean['Drug1_Normalized'] = df_ddi_clean['Drug 1'].apply(normalize_drug_name)\n",
    "df_ddi_clean['Drug2_Normalized'] = df_ddi_clean['Drug 2'].apply(normalize_drug_name)\n",
    "logging.info(\"Added normalized drug name columns\")\n",
    "\n",
    "# 6. Extract severity classification from interaction description\n",
    "def classify_severity(description):\n",
    "    \"\"\"Classify interaction severity based on keywords.\"\"\"\n",
    "    if pd.isna(description):\n",
    "        return 'Unknown'\n",
    "    desc_lower = description.lower()\n",
    "    if any(word in desc_lower for word in ['contraindicated', 'avoid', 'serious', 'severe']):\n",
    "        return 'High'\n",
    "    elif any(word in desc_lower for word in ['caution', 'monitor', 'may increase', 'may decrease']):\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df_ddi_clean['Severity'] = df_ddi_clean['Interaction Description'].apply(classify_severity)\n",
    "logging.info(\"Added severity classification column\")\n",
    "\n",
    "logging.info(f\"DDI cleaning complete: {len(df_ddi_clean):,} records\")\n",
    "\n",
    "print(f\"\\nCleaned DDI Data Shape: {df_ddi_clean.shape}\")\n",
    "df_ddi_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddi-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDI cleaning summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DDI DATASET - CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Original records:     {initial_count:,}\")\n",
    "print(f\"Cleaned records:      {len(df_ddi_clean):,}\")\n",
    "print(f\"Records removed:      {initial_count - len(df_ddi_clean):,}\")\n",
    "print(f\"Removal rate:         {((initial_count - len(df_ddi_clean)) / initial_count * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nSeverity distribution:\")\n",
    "print(df_ddi_clean['Severity'].value_counts())\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - Drug1_Normalized\")\n",
    "print(\"  - Drug2_Normalized\")\n",
    "print(\"  - Severity\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Clean Medications Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meds-initial-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data quality assessment for medications dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MEDICATIONS DATASET - INITIAL QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nShape: {df_meds_raw.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_meds_raw.isnull().sum())\n",
    "\n",
    "print(f\"\\nDuplicate rows: {df_meds_raw.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df_meds_raw.dtypes)\n",
    "\n",
    "print(f\"\\nSource system distribution:\")\n",
    "print(df_meds_raw['SourceSystem'].value_counts())\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meds-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean medications dataset\n",
    "\n",
    "logging.info(\"Cleaning medications dataset...\")\n",
    "\n",
    "# Create copy for cleaning\n",
    "df_meds_clean = df_meds_raw.copy()\n",
    "\n",
    "initial_count = len(df_meds_clean)\n",
    "logging.info(f\"Starting with {initial_count:,} records\")\n",
    "\n",
    "# 1. Remove exact duplicates\n",
    "df_meds_clean = df_meds_clean.drop_duplicates()\n",
    "duplicates_removed = initial_count - len(df_meds_clean)\n",
    "logging.info(f\"Removed {duplicates_removed:,} exact duplicate records\")\n",
    "\n",
    "# 2. Remove records with missing critical fields\n",
    "before_critical = len(df_meds_clean)\n",
    "df_meds_clean = df_meds_clean.dropna(subset=['PatientSID', 'DrugNameWithoutDose', 'MedicationDateTime'])\n",
    "critical_removed = before_critical - len(df_meds_clean)\n",
    "logging.info(f\"Removed {critical_removed:,} records with missing critical fields\")\n",
    "\n",
    "# 3. Ensure proper data types\n",
    "df_meds_clean['PatientSID'] = df_meds_clean['PatientSID'].astype('int64')\n",
    "df_meds_clean['MedicationDateTime'] = pd.to_datetime(df_meds_clean['MedicationDateTime'])\n",
    "df_meds_clean['StartDate'] = pd.to_datetime(df_meds_clean['StartDate'])\n",
    "logging.info(\"Ensured proper data types\")\n",
    "\n",
    "# 4. Strip whitespace from string columns\n",
    "string_cols = ['DrugNameWithoutDose', 'DrugNameWithDose', 'SourceSystem', 'Status']\n",
    "for col in string_cols:\n",
    "    if col in df_meds_clean.columns:\n",
    "        df_meds_clean[col] = df_meds_clean[col].astype(str).str.strip()\n",
    "logging.info(\"Stripped whitespace from string columns\")\n",
    "\n",
    "# 5. Add normalized drug name column\n",
    "def extract_base_drug_name(drug_name):\n",
    "    \"\"\"Extract base drug name: remove dose, normalize.\"\"\"\n",
    "    if pd.isna(drug_name):\n",
    "        return None\n",
    "    name = str(drug_name).upper().strip()\n",
    "    # Remove dose info (everything after first digit)\n",
    "    name = re.split(r'\\s*\\d', name)[0].strip()\n",
    "    # Apply normalization\n",
    "    return normalize_drug_name(name)\n",
    "\n",
    "df_meds_clean['DrugName_Normalized'] = df_meds_clean['DrugNameWithoutDose'].apply(extract_base_drug_name)\n",
    "logging.info(\"Added normalized drug name column\")\n",
    "\n",
    "# 6. Add date components for analysis\n",
    "df_meds_clean['MedicationYear'] = df_meds_clean['MedicationDateTime'].dt.year\n",
    "df_meds_clean['MedicationMonth'] = df_meds_clean['MedicationDateTime'].dt.month\n",
    "df_meds_clean['MedicationDayOfWeek'] = df_meds_clean['MedicationDateTime'].dt.dayofweek\n",
    "df_meds_clean['MedicationHour'] = df_meds_clean['MedicationDateTime'].dt.hour\n",
    "logging.info(\"Added date component columns\")\n",
    "\n",
    "# 7. Sort by patient and datetime\n",
    "df_meds_clean = df_meds_clean.sort_values(['PatientSID', 'MedicationDateTime'])\n",
    "logging.info(\"Sorted by patient and datetime\")\n",
    "\n",
    "# 8. Reset index\n",
    "df_meds_clean = df_meds_clean.reset_index(drop=True)\n",
    "\n",
    "logging.info(f\"Medications cleaning complete: {len(df_meds_clean):,} records\")\n",
    "\n",
    "print(f\"\\nCleaned Medications Data Shape: {df_meds_clean.shape}\")\n",
    "df_meds_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meds-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medications cleaning summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEDICATIONS DATASET - CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Original records:     {initial_count:,}\")\n",
    "print(f\"Cleaned records:      {len(df_meds_clean):,}\")\n",
    "print(f\"Records removed:      {initial_count - len(df_meds_clean):,}\")\n",
    "print(f\"Removal rate:         {((initial_count - len(df_meds_clean)) / initial_count * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nUnique patients:      {df_meds_clean['PatientSID'].nunique()}\")\n",
    "print(f\"Unique medications:   {df_meds_clean['DrugName_Normalized'].nunique()}\")\n",
    "\n",
    "print(f\"\\nSource system distribution:\")\n",
    "print(df_meds_clean['SourceSystem'].value_counts())\n",
    "\n",
    "print(f\"\\nDate range:\")\n",
    "print(f\"  Earliest: {df_meds_clean['MedicationDateTime'].min()}\")\n",
    "print(f\"  Latest:   {df_meds_clean['MedicationDateTime'].max()}\")\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - DrugName_Normalized\")\n",
    "print(\"  - MedicationYear\")\n",
    "print(\"  - MedicationMonth\")\n",
    "print(\"  - MedicationDayOfWeek\")\n",
    "print(\"  - MedicationHour\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q0hssyv597c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3.5: Clean Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7p839y5wee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data quality assessment for demographics dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DEMOGRAPHICS DATASET - INITIAL QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nShape: {df_demo_raw.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_demo_raw.isnull().sum())\n",
    "\n",
    "print(f\"\\nDuplicate rows: {df_demo_raw.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df_demo_raw.dtypes)\n",
    "\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(df_demo_raw['Gender'].value_counts())\n",
    "\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(df_demo_raw['Age'].describe())\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2iatd3xco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean demographics dataset\n",
    "\n",
    "logging.info(\"Cleaning demographics dataset...\")\n",
    "\n",
    "# Create copy for cleaning\n",
    "df_demo_clean = df_demo_raw.copy()\n",
    "\n",
    "initial_count = len(df_demo_clean)\n",
    "logging.info(f\"Starting with {initial_count:,} records\")\n",
    "\n",
    "# 1. Remove duplicates (should be one record per patient)\n",
    "df_demo_clean = df_demo_clean.drop_duplicates(subset=['PatientSID'])\n",
    "duplicates_removed = initial_count - len(df_demo_clean)\n",
    "logging.info(f\"Removed {duplicates_removed:,} duplicate patient records\")\n",
    "\n",
    "# 2. Remove records with missing critical fields\n",
    "before_critical = len(df_demo_clean)\n",
    "df_demo_clean = df_demo_clean.dropna(subset=['PatientSID', 'DateOfBirth', 'Gender'])\n",
    "critical_removed = before_critical - len(df_demo_clean)\n",
    "logging.info(f\"Removed {critical_removed:,} records with missing critical fields\")\n",
    "\n",
    "# 3. Ensure proper data types\n",
    "df_demo_clean['PatientSID'] = df_demo_clean['PatientSID'].astype('int64')\n",
    "df_demo_clean['DateOfBirth'] = pd.to_datetime(df_demo_clean['DateOfBirth'])\n",
    "logging.info(\"Ensured proper data types\")\n",
    "\n",
    "# 4. Standardize Gender values\n",
    "df_demo_clean['Gender'] = df_demo_clean['Gender'].str.strip().str.upper()\n",
    "# Map common variations\n",
    "gender_mapping = {\n",
    "    'M': 'M', 'MALE': 'M', \n",
    "    'F': 'F', 'FEMALE': 'F',\n",
    "    'U': 'U', 'UNKNOWN': 'U'\n",
    "}\n",
    "df_demo_clean['Gender'] = df_demo_clean['Gender'].map(gender_mapping).fillna('U')\n",
    "logging.info(\"Standardized gender values\")\n",
    "\n",
    "# 5. Validate and clean Age\n",
    "# Recalculate age to ensure consistency\n",
    "today = pd.Timestamp.now()\n",
    "df_demo_clean['Age'] = ((today - df_demo_clean['DateOfBirth']).dt.days / 365.25).astype(int)\n",
    "\n",
    "# Flag unrealistic ages\n",
    "unrealistic_ages = (df_demo_clean['Age'] < 0) | (df_demo_clean['Age'] > 120)\n",
    "if unrealistic_ages.sum() > 0:\n",
    "    logging.warning(f\"Found {unrealistic_ages.sum()} records with unrealistic ages\")\n",
    "    df_demo_clean = df_demo_clean[~unrealistic_ages]\n",
    "    logging.info(f\"Removed {unrealistic_ages.sum()} records with unrealistic ages\")\n",
    "\n",
    "logging.info(\"Age validated and recalculated\")\n",
    "\n",
    "# 6. Add age groups for analysis\n",
    "df_demo_clean['AgeGroup'] = pd.cut(\n",
    "    df_demo_clean['Age'], \n",
    "    bins=[0, 18, 40, 65, 80, 120],\n",
    "    labels=['<18', '18-39', '40-64', '65-79', '80+']\n",
    ")\n",
    "logging.info(\"Added age group categories\")\n",
    "\n",
    "# 7. Add is_elderly flag (clinical cutoff: 65+)\n",
    "df_demo_clean['IsElderly'] = (df_demo_clean['Age'] >= 65).astype(int)\n",
    "logging.info(\"Added elderly flag\")\n",
    "\n",
    "# 8. Sort by PatientSID\n",
    "df_demo_clean = df_demo_clean.sort_values('PatientSID').reset_index(drop=True)\n",
    "\n",
    "logging.info(f\"Demographics cleaning complete: {len(df_demo_clean):,} records\")\n",
    "\n",
    "print(f\"\\nCleaned Demographics Data Shape: {df_demo_clean.shape}\")\n",
    "df_demo_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4vjwubrdn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics cleaning summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMOGRAPHICS DATASET - CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Original records:     {initial_count:,}\")\n",
    "print(f\"Cleaned records:      {len(df_demo_clean):,}\")\n",
    "print(f\"Records removed:      {initial_count - len(df_demo_clean):,}\")\n",
    "print(f\"Removal rate:         {((initial_count - len(df_demo_clean)) / initial_count * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(df_demo_clean['Gender'].value_counts())\n",
    "\n",
    "print(f\"\\nAge group distribution:\")\n",
    "print(df_demo_clean['AgeGroup'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nElderly patients (65+): {df_demo_clean['IsElderly'].sum()} ({df_demo_clean['IsElderly'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(df_demo_clean['Age'].describe())\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(\"  - AgeGroup\")\n",
    "print(\"  - IsElderly\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Write Cleaned Data to v2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-ddi-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned DDI dataset to v2_clean\n",
    "\n",
    "ddi_clean_filename = \"db_drug_interactions_clean.parquet\"\n",
    "ddi_clean_uri = f\"s3://{DEST_BUCKET}/{V2_CLEAN_DDI_PREFIX}{ddi_clean_filename}\"\n",
    "logging.info(f\"Writing cleaned DDI data: {ddi_clean_uri}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_ddi_clean.to_parquet(\n",
    "    ddi_clean_uri,\n",
    "    engine='pyarrow',\n",
    "    filesystem=fs,\n",
    "    compression='snappy',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logging.info(f\"Successfully wrote {len(df_ddi_clean):,} records in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"✓ DDI clean data written to: {ddi_clean_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-meds-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned medications dataset to v2_clean\n",
    "\n",
    "meds_clean_filename = \"medications_clean.parquet\"\n",
    "meds_clean_uri = f\"s3://{DEST_BUCKET}/{V2_CLEAN_MEDICATIONS_PREFIX}{meds_clean_filename}\"\n",
    "logging.info(f\"Writing cleaned medications data: {meds_clean_uri}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_meds_clean.to_parquet(\n",
    "    meds_clean_uri,\n",
    "    engine='pyarrow',\n",
    "    filesystem=fs,\n",
    "    compression='snappy',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logging.info(f\"Successfully wrote {len(df_meds_clean):,} records in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"✓ Medications clean data written to: {meds_clean_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aj1vgavuk0s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned demographics dataset to v2_clean\n",
    "\n",
    "demo_clean_filename = \"patient_demographics_clean.parquet\"\n",
    "demo_clean_uri = f\"s3://{DEST_BUCKET}/v2_clean/demographics/{demo_clean_filename}\"\n",
    "logging.info(f\"Writing cleaned demographics data: {demo_clean_uri}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_demo_clean.to_parquet(\n",
    "    demo_clean_uri,\n",
    "    engine='pyarrow',\n",
    "    filesystem=fs,\n",
    "    compression='snappy',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logging.info(f\"Successfully wrote {len(df_demo_clean):,} records in {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"✓ Demographics clean data written to: {demo_clean_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-ddi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DDI clean data by reading back\n",
    "\n",
    "logging.info(\"Verifying DDI clean data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_ddi_verify = pd.read_parquet(ddi_clean_uri, filesystem=fs)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "assert len(df_ddi_verify) == len(df_ddi_clean), \"Row count mismatch!\"\n",
    "assert len(df_ddi_verify.columns) == len(df_ddi_clean.columns), \"Column count mismatch!\"\n",
    "\n",
    "logging.info(f\"✓ DDI verification successful: {len(df_ddi_verify):,} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(\"\\nDDI Clean Data (first 5 rows):\")\n",
    "df_ddi_verify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-meds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify medications clean data by reading back\n",
    "\n",
    "logging.info(\"Verifying medications clean data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_meds_verify = pd.read_parquet(meds_clean_uri, filesystem=fs)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "assert len(df_meds_verify) == len(df_meds_clean), \"Row count mismatch!\"\n",
    "assert len(df_meds_verify.columns) == len(df_meds_clean.columns), \"Column count mismatch!\"\n",
    "\n",
    "logging.info(f\"✓ Medications verification successful: {len(df_meds_verify):,} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(\"\\nMedications Clean Data (first 5 rows):\")\n",
    "df_meds_verify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mniao6okpz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify demographics clean data by reading back\n",
    "\n",
    "logging.info(\"Verifying demographics clean data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_demo_verify = pd.read_parquet(demo_clean_uri, filesystem=fs)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "assert len(df_demo_verify) == len(df_demo_clean), \"Row count mismatch!\"\n",
    "assert len(df_demo_verify.columns) == len(df_demo_clean.columns), \"Column count mismatch!\"\n",
    "\n",
    "logging.info(f\"✓ Demographics verification successful: {len(df_demo_verify):,} rows in {elapsed:.2f}s\")\n",
    "\n",
    "print(\"\\nDemographics Clean Data (first 5 rows):\")\n",
    "df_demo_verify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDDI REFERENCE DATASET:\")\n",
    "print(f\"  Input:  s3://{DEST_BUCKET}/{V1_RAW_DDI_PREFIX}db_drug_interactions.parquet\")\n",
    "print(f\"  Output: s3://{DEST_BUCKET}/{V2_CLEAN_DDI_PREFIX}{ddi_clean_filename}\")\n",
    "print(f\"  Records: {len(df_ddi_raw):,} → {len(df_ddi_clean):,}\")\n",
    "print(f\"  Columns: {len(df_ddi_raw.columns)} → {len(df_ddi_clean.columns)}\")\n",
    "print(f\"  Status: ✓ Complete\")\n",
    "\n",
    "print(\"\\nMEDICATIONS DATASET:\")\n",
    "print(f\"  Input:  s3://{DEST_BUCKET}/{V1_RAW_MEDICATIONS_PREFIX}medications_combined.parquet\")\n",
    "print(f\"  Output: s3://{DEST_BUCKET}/{V2_CLEAN_MEDICATIONS_PREFIX}{meds_clean_filename}\")\n",
    "print(f\"  Records: {len(df_meds_raw):,} → {len(df_meds_clean):,}\")\n",
    "print(f\"  Columns: {len(df_meds_raw.columns)} → {len(df_meds_clean.columns)}\")\n",
    "print(f\"  Status: ✓ Complete\")\n",
    "\n",
    "print(\"\\nDEMOGRAPHICS DATASET:\")\n",
    "print(f\"  Input:  s3://{DEST_BUCKET}/v1_raw/demographics/patient_demographics.parquet\")\n",
    "print(f\"  Output: s3://{DEST_BUCKET}/v2_clean/demographics/{demo_clean_filename}\")\n",
    "print(f\"  Records: {len(df_demo_raw):,} → {len(df_demo_clean):,}\")\n",
    "print(f\"  Columns: {len(df_demo_raw.columns)} → {len(df_demo_clean.columns)}\")\n",
    "print(f\"  Status: ✓ Complete\")\n",
    "\n",
    "print(\"\\nCLEANING OPERATIONS APPLIED:\")\n",
    "print(\"  ✓ Removed duplicate records\")\n",
    "print(\"  ✓ Removed records with missing critical values\")\n",
    "print(\"  ✓ Stripped whitespace from text fields\")\n",
    "print(\"  ✓ Ensured proper data types\")\n",
    "print(\"  ✓ Added normalized drug names for matching\")\n",
    "print(\"  ✓ Added derived columns (severity, date components, age groups)\")\n",
    "print(\"  ✓ Sorted and indexed data\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  → Run 04_features.ipynb (demographics will be joined into patient features)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
