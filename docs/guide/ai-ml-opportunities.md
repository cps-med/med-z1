# med-z1 AI/ML Assessment and Enhancement Opportunities

**Document Version:** v1.1  
**Created:** 2026-02-05  
**Updated:** 2026-02-05  
**Focus:** Predictive Analytics for Risk Stratification and Clinical Decision Support  
**Target ML Technologies:** Traditional ML (scikit-learn, XGBoost)  

**Changelog:**
- **v1.1 (2026-02-05):**
  - Added Section 1.3: Reusable Patterns from MCP Subsystem (5 key patterns for ML integration)
  - Added Phase 3: Suicide Prevention Risk Model (REACH VET 2.0) - Second ML use case after readmission
  - Renumbered original Phase 3-5 to Phase 4-6, maintaining all planned models
  - Updated timeline: Total roadmap now 22 weeks (was 18 weeks)
- **v1.0 (2026-02-05):**
  - Initial assessment with 6 recommended ML models and 5-phase roadmap

---

## Executive Summary

This assessment evaluates the current state of AI/ML capabilities in med-z1 and identifies opportunities to enhance the product with predictive analytics focused on risk stratification, readmission prediction, and care gap identification using traditional machine learning approaches.

Med-z1 has the data and infrastructure to implement high-value predictive ML models that could significantly improve clinical outcomes, reduce readmissions, and identify high-risk patients proactively.

---

## Table of Contents

1. [Current AI/ML State](#1-current-aiml-state)
2. [Available Data Assets](#2-available-data-assets)
3. [Gaps and Limitations](#3-gaps-and-limitations)
4. [Recommended ML Enhancements](#4-recommended-ml-enhancements)
5. [Implementation Roadmap](#5-implementation-roadmap)
6. [Technical Architecture](#6-technical-architecture)
7. [Model Development Guidelines](#7-model-development-guidelines)
8. [Success Metrics](#8-success-metrics)
9. [Risk Mitigation](#9-risk-mitigation)

---

## 1. Current AI/ML State

### 1.1 Implemented AI Capabilities

**A. LLM-Based Conversational AI (Phase 1-6 Complete)**

**Technology Stack:**
- LangGraph 1.0.5 (Agentic RAG orchestration)
- OpenAI GPT-4-turbo (reasoning and synthesis)
- PostgreSQL conversation memory (checkpoints)
- FastAPI + HTMX UI integration

**Available AI Tools (4):**
1. **`check_ddi_risks`** - Drug-drug interaction analysis
   - Uses DrugBank reference data (~191K interactions)
   - Rule-based lookup (not ML-based)
   - Real-time analysis of active medications

2. **`get_patient_summary`** - Comprehensive patient overview
   - Aggregates demographics, meds, vitals, allergies, encounters, notes
   - LLM synthesis of structured data into narrative
   - No predictive component

3. **`analyze_vitals_trends`** - Statistical vitals analysis
   - **Uses:** NumPy `np.polyfit()` for linear regression (trend lines only)
   - **Purpose:** Visualization and descriptive statistics
   - **NOT predictive:** No outcome prediction or risk scoring

4. **`get_clinical_notes_summary`** - Clinical notes query and summarization
   - LLM-based text summarization
   - No NLP entity extraction or predictive insights

**Strengths:**
- Excellent conversational interface for clinician questions
- Real-time data integration (VistA RPC + PostgreSQL)
- Transparent data provenance and attribution
- Production-ready web UI at `/insight`

**Limitations:**
- Purely reactive (answers questions but doesn't proactively predict risks)
- No patient outcome prediction
- No risk stratification models
- No care gap prediction

---

**B. MCP Servers for External AI Clients (2 Servers)**

**1. EHR Data Server** (`mcpsvr/ehr_server.py`)
- Exposes patient clinical data to Claude Desktop
- 5 tools: patient summary, medications, vitals, allergies, encounters
- Read-only access to PostgreSQL data
- No ML/predictive capabilities

**2. Clinical Decision Support Server** (`mcpsvr/clinical_decision_support_server.py`)
- 4 clinical algorithms:
  - `check_drug_interactions` - DDI analysis (rule-based)
  - `assess_fall_risk` - Fall risk scoring (rule-based heuristics)
  - `calculate_ckd_egfr` - CKD-EPI eGFR calculation (formula-based)
  - `recommend_cancer_screening` - USPSTF guideline matching (rule-based)

**Strengths:**
- Well-architected clinical algorithms with proper attribution
- Reuses existing business logic from `ai/services/`
- Excellent data provenance documentation

**Limitations:**
- All algorithms are **rule-based** (not ML-based)
- Fall risk scoring uses simple heuristics (age + medication count)
- No predictive modeling or risk stratification beyond simple scoring

---

### 1.2 Current Use of scikit-learn

**Installation:**
- `scikit-learn>=1.3.0` present in `requirements.txt`
- Comment: "Data preprocessing, metrics, train/test split"

**Actual Usage:**
- **ZERO** - Not used anywhere in the codebase
- No model training scripts
- No predictive models
- No model persistence/loading
- No train/test splitting

**NumPy Usage:**
- Limited to `np.polyfit()` for linear regression (trend visualization)
- Basic statistics: `np.mean()`, `np.max()`, `np.sum()`
- No feature engineering or predictive modeling

**Conclusion:** scikit-learn is installed but completely unutilized for predictive analytics.

---

### 1.3 Reusable Patterns from MCP Subsystem

The **MCP servers** (`mcpsvr/`) provide excellent architectural patterns that can be leveraged for ML model integration. These servers demonstrate best practices for clinical algorithm deployment that should be adopted for predictive models.

#### Pattern #1: Data Provenance & Attribution

**Source:** `mcpsvr/clinical_decision_support_server.py:95-135`

Both MCP servers use `_add_attribution_footer()` to provide clinical auditability:

```python
def _add_attribution_footer(
    result: str,
    tool_name: str,
    data_sources: list[str],
    metadata: dict = None
) -> str:
    """
    Shows exactly what data sources and tools were used.
    Critical for clinical auditability and regulatory compliance.
    """
    footer = "\n\n---\n\n**Data Provenance:**\n"
    footer += f"  â€¢ **Tool:** `{tool_name}`\n"
    footer += f"  â€¢ **Data Sources:** {', '.join(data_sources)}\n"
    footer += f"  â€¢ **Analysis Timestamp:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\n"
    # ... metadata
    return result + footer
```

**Application to Readmission Model:**
- Display which PostgreSQL tables were queried
- Show feature count and model version
- Include model performance: "Model: XGBoost v1.2, AUC: 0.78"
- Timestamp when prediction was generated

---

#### Pattern #2: Explainability (Contributing Factors)

**Source:** `mcpsvr/clinical_decision_support_server.py:437-451` (fall risk assessment)

MCP servers show **which factors contribute to scores**:

```python
if factors:
    result += "Contributing Factors:\n"
    for factor in factors:
        result += f"  â€¢ {factor}\n"
```

**Application to Readmission Model:**
Use SHAP values to show feature contributions:

```
Contributing to 42% readmission risk:
  â€¢ 3 ED visits in last 6 months (+12% risk)
  â€¢ Polypharmacy - 14 active medications (+8% risk)
  â€¢ Recent weight loss - 10 lbs in 30 days (+7% risk)
  â€¢ Chronic pain diagnosis (+5% risk)
  â€¢ Missed 2 appointments in last 90 days (+4% risk)
```

This format is already validated in production (MCP servers) and trusted by clinicians.

---

#### Pattern #3: Risk Tier Classification

**Source:** `mcpsvr/clinical_decision_support_server.py:427-436` (fall risk tiers)

MCP servers use 3-tier risk classification with visual indicators:

```python
if risk_score <= 1:
    risk_level = "LOW"
    icon = "ðŸŸ¢"
elif risk_score <= 3:
    risk_level = "MODERATE"
    icon = "ðŸŸ¡"
else:
    risk_level = "HIGH"
    icon = "ðŸ”´"
```

**Application to All ML Models:**
- **Readmission:** LOW (<20%), MODERATE (20-40%), HIGH (>40%)
- **Suicide Risk:** Standard, Moderate, High (per REACH VET 2.0 tiers)
- **Fall Risk (ML):** LOW (<10%), MODERATE (10-25%), HIGH (>25%)

Consistent tier display across all models: `ðŸ”´ HIGH - 42% readmission risk`

---

#### Pattern #4: Clinical Recommendations

**Source:** `mcpsvr/clinical_decision_support_server.py:449-461, 573-581`

Each MCP assessment provides **actionable recommendations**:

```python
result += "\n**Recommendations:**\n"
if risk_level == "HIGH":
    result += "  â€¢ Implement fall prevention protocols\n"
    result += "  â€¢ Review medications for dose reduction\n"
elif risk_level == "MODERATE":
    result += "  â€¢ Monitor for fall risk factors\n"
```

**Application to Readmission Model:**
```
**Recommendations:**
HIGH Risk (42%):
  â€¢ Enroll in care coordination program (Transition of Care team)
  â€¢ Schedule 72-hour post-discharge follow-up appointment
  â€¢ Verify patient understands discharge instructions
  â€¢ Consider medication reconciliation with pharmacist
  â€¢ Provide patient with 24/7 clinic contact number

MODERATE Risk (28%):
  â€¢ Schedule 7-day post-discharge phone check-in
  â€¢ Ensure follow-up appointment scheduled before discharge
  â€¢ Verify patient has transportation to appointments

LOW Risk (12%):
  â€¢ Routine follow-up per standard discharge protocol
```

---

#### Pattern #5: Clinical Safety Disclaimers

**Source:** `mcpsvr/clinical_decision_support_server.py:572-581` (CKD-eGFR recommendations)

MCP tools include clinical context and limitations:

```python
if egfr < 60:
    result += "**Clinical Recommendations:**\n"
    result += "  â€¢ Confirm with repeat testing (GFR varies ~5%)\n"
```

**Application to All ML Models:**

```
**Clinical Disclaimer:**
This prediction is a clinical decision support tool, not a diagnosis.
Risk score should supplement, not replace, clinical judgment.
Consider patient-specific factors not captured by the model:
  â€¢ Social support and discharge disposition
  â€¢ Patient functional status and mobility
  â€¢ Caregiver availability and health literacy
  â€¢ Recent medication or treatment plan changes

Confidence: 42% (95% CI: 35-49%)
Model: XGBoost v1.2, trained on 2500 encounters
Performance: AUC-ROC 0.78, validated 2026-01
```

---

#### Summary: MCP Patterns for ML Integration

| Pattern | MCP Server Source | ML Model Application | Benefit |
|---------|------------------|---------------------|---------|
| **Data Provenance** | Attribution footer | Show data sources, model version, timestamp | Regulatory compliance, audit trail |
| **Explainability** | Contributing factors list | SHAP feature contributions with % impact | Clinician trust, actionable insights |
| **Risk Tiers** | 3-tier with emoji icons | Standardized LOW/MODERATE/HIGH classification | Consistent UX across all models |
| **Recommendations** | Tier-specific actions | Context-specific care coordination steps | Workflow integration, actionable |
| **Disclaimers** | Clinical limitations | Model confidence intervals, update dates | Medical-legal protection, transparency |

**Implementation:** Create `ai/ml/formatting/` module with shared formatting functions that implement these patterns for all ML models.

---

## 2. Available Data Assets

med-z1 has **excellent data infrastructure** for training predictive ML models:

### 2.1 PostgreSQL Clinical Database (`medz1`)

**12 Clinical Tables in `clinical` schema:**

| Table | Records | Key Fields | ML Potential |
|-------|---------|------------|--------------|
| `patient_demographics` | ~50 patients | Age, sex, DOB, service-connected % | **High** - Age, demographics predict readmissions |
| `patient_vitals` | ~1000+ vitals | BP, HR, temp, weight, BMI | **Very High** - Vital trends predict deterioration |
| `patient_medications_outpatient` | ~200+ meds | Drug names, sig, dates | **High** - Polypharmacy predicts readmissions |
| `patient_medications_inpatient` | ~150+ meds | Drug names, sig, dates | **High** - Medication changes during admits |
| `patient_encounters` | ~100+ encounters | Admit date, discharge, category | **Very High** - Encounter patterns predict readmissions |
| `patient_allergies` | ~100+ allergies | Allergen, reaction, severity | **Medium** - Safety risk factor |
| `patient_labs` | ~58 results | Lab name, result value, date | **Very High** - Labs predict clinical deterioration |
| `patient_clinical_notes` | 106 notes | Progress notes, consults, discharge summaries | **Very High** - NLP for unstructured insights |
| `patient_immunizations` | 138 immunizations | Vaccine history, CVX codes | **Medium** - Vaccine compliance predicts ED visits |
| `patient_flags` | Active flags | Cat I/II flags, review dates | **High** - High-risk flags predict adverse events |
| `patient_flag_history` | Audit trail | Flag changes over time | **Medium** - Historical risk patterns |
| `patient_allergy_reactions` | Reactions | Detailed allergy reactions | **Low** - Safety data, less predictive |

**Data Volume:** Sufficient for proof-of-concept ML models (100+ patients needed for robust validation)

---

### 2.2 Real-Time VistA Data (T-0, Current Day)

**4 Operational VistA RPC Endpoints:**
1. Vitals - `GMV LATEST VM`
2. Encounters - `ORWCV ADMISSIONS`
3. Allergies - `ORQQAL LIST`
4. Medications - `ORWPS COVER`

**ML Opportunity:**
- Real-time vitals can trigger early warning scores (EWS)
- Current-day ED visits can trigger readmission alerts
- Fresh medication changes can trigger DDI or fall risk recalculations

---

### 2.3 Reference Data

**DDI Reference:**
- ~191K drug-drug interactions from DrugBank
- Currently used for rule-based lookups
- **ML Opportunity:** Train severity prediction model based on interaction descriptions

**Vaccine Reference:**
- 30 CVX vaccine codes with descriptions
- **ML Opportunity:** Predict vaccine non-compliance risk

---

### 2.4 Temporal Coverage

**Historical Data (PostgreSQL):** T-1 and earlier (typically 1-3 years)
**Real-Time Data (VistA):** T-0 (current day)

**ML Opportunity:** Time-series models can use longitudinal data to predict future events (30-day readmission, 7-day deterioration)

---

## 3. Gaps and Limitations

### 3.1 No Predictive Modeling

**Current State:**
- All AI capabilities are **reactive** (answer questions after clinician asks)
- No proactive risk alerts or predictions
- No patient outcome forecasting

**Impact:**
- Clinicians must remember to ask about risks (cognitive burden)
- High-risk patients may not be identified until too late
- No early warning system for clinical deterioration

---

### 3.2 No Risk Stratification

**Current State:**
- Fall risk assessment uses simple heuristics (age â‰¥65 = +2 points)
- No multi-variate risk models
- No calibrated probability scores (e.g., "35% risk of 30-day readmission")

**Impact:**
- Cannot prioritize care management resources to highest-risk patients
- No objective risk tiers (low/medium/high based on validated models)
- No integration with care coordination workflows

---

### 3.3 No Model Training Infrastructure

**Current State:**
- No scripts for training models on historical data
- No model versioning or persistence
- No model evaluation (AUC, precision/recall, calibration)
- No train/test splitting or cross-validation

**Impact:**
- Cannot leverage 1-3 years of historical outcomes data
- No continuous model improvement as data accumulates
- No regulatory-compliant model validation

---

### 3.4 Limited Feature Engineering

**Current State:**
- Raw data fields used directly (e.g., systolic BP, heart rate)
- No derived features (e.g., "BP increased >20 mmHg in 7 days")
- No time-windowed aggregations (e.g., "3+ ED visits in last 6 months")

**Impact:**
- Missing powerful predictive signals from temporal patterns
- Cannot capture polypharmacy risk (e.g., â‰¥10 medications)
- Cannot detect care gaps (e.g., "overdue for mammogram")

---

### 3.5 No Integration with Clinical Workflows

**Current State:**
- AI Insights is a **separate page** (`/insight` chatbot)
- No proactive risk alerts on patient dashboard
- No risk scores displayed on encounter lists

**Impact:**
- Clinicians must actively seek out AI insights (low adoption)
- No passive decision support at point of care
- Risk information siloed from main workflows

---

## 4. Recommended ML Enhancements

Based on your priorities (risk stratification, high-risk patients, medication management, care gaps), here are **6 high-value ML models** to implement using traditional ML (scikit-learn, XGBoost):

---

### 4.1 **30-Day Hospital Readmission Risk Model**

**Priority: HIGHEST**

**Business Value:**
- Medicare penalties for excess readmissions (~$563M annually across VA system)
- Early identification enables care coordination interventions
- Reduces patient morbidity and healthcare costs

**Model Type:** Binary classification (readmitted within 30 days: Yes/No)

**Features (30-40):**
1. **Demographics:** Age, sex, service-connected %
2. **Prior utilization:** Count of ED visits (6 months), inpatient admits (1 year)
3. **Clinical complexity:** Active medication count, allergy count, active flag count
4. **Vitals instability:** BP standard deviation (30 days), weight change % (30 days)
5. **Discharge factors:** Length of stay, discharge disposition
6. **Lab abnormalities:** Creatinine >1.5, eGFR <45, anemia (Hgb <10)
7. **Medications:** Diuretic use, anticoagulant use, polypharmacy (â‰¥10 meds)
8. **Social factors:** Service-connected %, distance from facility (if available)

**Target Variable:** Readmitted within 30 days of discharge (binary)

**Training Data:**
- Historical encounters with outcomes (readmitted or not)
- Minimum 500 encounters for robust model
- Need discharge dates + subsequent admission dates

**Algorithms to Test:**
1. Logistic Regression (baseline, interpretable)
2. Random Forest (handles non-linear interactions)
3. **XGBoost** (best performance for tabular data)
4. Gradient Boosting (scikit-learn)

**Expected Performance:**
- AUC-ROC: 0.70-0.80 (good discrimination)
- Top 10% risk tier: 40-50% positive predictive value

**Implementation Effort:** 3-4 weeks
- Week 1: Feature engineering, train/test split
- Week 2: Model training, hyperparameter tuning
- Week 3: Evaluation, calibration, threshold selection
- Week 4: Integration into dashboard + testing

---

### 4.2 **High-Risk Patient Identification Score**

**Priority: HIGHEST**

**Business Value:**
- Identify top 5% of patients who consume 50% of resources
- Enables proactive care management enrollment
- Reduces preventable ED visits and admissions

**Model Type:** Multi-output prediction
- Risk of ED visit (next 90 days)
- Risk of hospitalization (next 90 days)
- Predicted healthcare costs (next 12 months)

**Features (40-50):**
1. **Utilization history:** ED visits (6m, 1y), admits (1y), readmits (1y)
2. **Chronic conditions:** Count of active patient flags (Cat I/II)
3. **Medication burden:** Active med count, high-risk meds (opioids, benzos, anticoagulants)
4. **Clinical instability:** Recent weight change %, BP variability, lab abnormalities
5. **Care gaps:** Overdue labs, missed appointments (if available)
6. **Social determinants:** Service-connected %, homelessness flag (if available)
7. **Frailty indicators:** Age â‰¥75, fall risk score, polypharmacy

**Target Variables:**
- ED visit within 90 days (binary)
- Hospital admission within 90 days (binary)
- Total costs next 12 months (continuous, log-transformed)

**Composite Score:** Weighted average of 3 predictions â†’ Single risk tier (0-100)

**Algorithms:**
- XGBoost for binary outcomes (ED/admit prediction)
- Random Forest or XGBoost for cost prediction
- Ensemble model for final composite score

**Expected Performance:**
- AUC-ROC: 0.75-0.85 for ED/admit prediction
- Top 5% risk tier: Captures 30-40% of future admissions

**Implementation Effort:** 4-5 weeks

---

### 4.3 **Medication Adherence Risk Model**

**Priority:** â­â­â­â­

**Business Value:**
- Non-adherence causes 125,000 deaths/year, $300B costs
- Predicts which patients will stop taking critical medications
- Enables pharmacist outreach and adherence interventions

**Model Type:** Binary classification (non-adherent: Yes/No)

**Features (20-30):**
1. **Medication complexity:** Total med count, dosing frequency (QID vs QD)
2. **Prior adherence:** Refill gaps (if available in VistA), late refills
3. **Clinical factors:** Depression diagnosis, cognitive impairment
4. **Polypharmacy:** â‰¥10 medications (adherence drops with complexity)
5. **Cost barriers:** Co-pay tier (if available), service-connected %
6. **Demographics:** Age <30 or >75 (higher non-adherence)

**Target Variable:** Non-adherent at 6 months (missing â‰¥2 refills or <80% PDC)

**Training Data:**
- Requires medication refill history (not currently in PostgreSQL)
- **Data Gap:** Would need VistA Pharmacy data (Prescription table) added to ETL

**Algorithms:**
- Logistic Regression (interpretable for pharmacists)
- Random Forest (handles missing refill data)

**Expected Performance:**
- AUC-ROC: 0.65-0.75 (moderate discrimination)
- Top 20% risk tier: 50-60% will be non-adherent

**Implementation Effort:** 3 weeks (if refill data available)

---

### 4.4 **Early Warning Score (Clinical Deterioration)**

**Priority:** â­â­â­â­

**Business Value:**
- Detects patients at risk of decompensation in next 24-72 hours
- Enables early intervention (ICU transfer, fluids, antibiotics)
- Reduces in-hospital mortality and ICU days

**Model Type:** Binary classification (deterioration: Yes/No)

**Features (15-25):**
1. **Vital signs:** Current BP, HR, temp, respiratory rate, SpO2
2. **Vital trends:** BP change (24h), HR trend (slope), temp max (24h)
3. **Labs:** Creatinine, WBC, lactate, Hgb, platelets (if available)
4. **Clinical context:** Age, active infections, surgery within 7 days
5. **Medications:** Vasopressor use, diuretic changes

**Target Variable:** Deterioration within 24 hours (ICU transfer, rapid response, death)

**Training Data:**
- Requires ICU transfer timestamps (not currently in PostgreSQL)
- **Data Gap:** Need to link encounters to ICU admissions

**Algorithms:**
- Logistic Regression (baseline, meets regulatory standards for EWS)
- XGBoost (higher performance)

**Expected Performance:**
- AUC-ROC: 0.75-0.85 (comparable to NEWS2, MEWS scores)
- Sensitivity at 90% specificity: 30-40%

**Implementation Effort:** 4 weeks

---

### 4.5 **Care Gap Prediction Model**

**Priority:** â­â­â­â­

**Business Value:**
- Identifies patients overdue for preventive services (mammogram, colonoscopy, A1C)
- Improves quality metrics and HEDIS scores
- Reduces preventable disease progression (e.g., late-stage cancer)

**Model Type:** Multi-label classification (care gaps: [colon_screen, breast_screen, diabetes_control, etc.])

**Features (25-35):**
1. **Demographics:** Age, sex (determines screening eligibility)
2. **Chronic conditions:** Diabetes, hypertension (determines due labs)
3. **Prior screening:** Time since last mammogram, colonoscopy (if available)
4. **Lab history:** A1C dates, LDL dates (for diabetes/CVD control)
5. **Utilization:** Recent PCP visits (higher engagement = less likely to have gaps)
6. **Social factors:** Service-connected % (higher SC = more engaged)

**Target Variables:** Multiple binary outcomes (one per care gap type)
- Overdue for colonoscopy (age 45-75, no scope in 10 years)
- Overdue for mammogram (female, age 50-74, no mammo in 2 years)
- Poor diabetes control (HbA1c >9 or no A1C in 6 months)
- Overdue for lipid panel (CVD risk, no LDL in 12 months)

**Training Data:**
- **Data Gap:** Need procedure history (colonoscopy, mammogram) from CDW
- **Partial Available:** Lab history (`patient_labs` table has ~58 results)

**Algorithms:**
- Multi-label Random Forest
- XGBoost with multi-output wrapper

**Expected Performance:**
- Per-gap AUC-ROC: 0.70-0.80
- Precision at 50% recall: 60-70%

**Implementation Effort:** 3-4 weeks

---

### 4.6 **Fall Risk Prediction Model (ML-Based)**

**Priority:** â­â­â­

**Business Value:**
- Falls cause 800,000 hospitalizations/year, $50B costs
- Current rule-based fall risk tool is simplistic (age + med count)
- ML model can improve discrimination and reduce false positives

**Model Type:** Binary classification (fall within 90 days: Yes/No)

**Features (20-30):**
1. **Demographics:** Age, sex
2. **Medications:** Benzodiazepines, sedatives, antihypertensives, diuretics, opioids, antipsychotics
3. **Polypharmacy:** Total med count, high-risk med count
4. **Clinical factors:** Recent weight loss, gait instability (if documented in notes)
5. **Vitals:** Orthostatic hypotension (BP drop when standing, if available)
6. **Prior falls:** Fall history (if available in encounter/note data)
7. **Cognitive impairment:** Dementia diagnosis, confusion noted in encounters

**Target Variable:** Fall within 90 days (binary, requires fall event data)

**Training Data:**
- **Data Gap:** Need fall event data (usually in nursing notes, incident reports)
- Could use proxy: ED visits for fall-related injuries (hip fracture, head injury)

**Algorithms:**
- Logistic Regression (baseline, interpretable for nursing)
- Random Forest (handles complex medication interactions)
- XGBoost (best performance)

**Expected Performance:**
- AUC-ROC: 0.70-0.75 (better than STRATIFY or Morse Fall Scale)
- Top 10% risk tier: 30-40% will fall

**Implementation Effort:** 3-4 weeks

---

## 5. Implementation Roadmap

### Phase 1: Foundation (Weeks 1-3)

**Goals:** Establish ML development infrastructure

**Tasks:**
1. Create `ai/ml/` module for predictive models
   ```
   ai/ml/
   â”œâ”€â”€ __init__.py
   â”œâ”€â”€ models/              # Trained model artifacts
   â”œâ”€â”€ training/            # Training scripts
   â”‚   â”œâ”€â”€ train_readmission.py
   â”‚   â”œâ”€â”€ train_highrisk.py
   â”‚   â””â”€â”€ utils.py
   â”œâ”€â”€ features/            # Feature engineering
   â”‚   â”œâ”€â”€ clinical_features.py
   â”‚   â”œâ”€â”€ temporal_features.py
   â”‚   â””â”€â”€ risk_scores.py
   â”œâ”€â”€ evaluation/          # Model evaluation
   â”‚   â””â”€â”€ metrics.py
   â””â”€â”€ inference/           # Prediction service
       â””â”€â”€ predictor.py
   ```

2. Add ML dependencies to `requirements.txt`:
   ```
   xgboost>=2.0.0           # Gradient boosting
   imbalanced-learn>=0.11.0 # Handle class imbalance
   shap>=0.43.0             # Model explainability
   mlflow>=2.9.0            # Model versioning (optional)
   ```

3. Create data extraction scripts:
   - `scripts/ml/extract_readmission_data.py` - Build training dataset from PostgreSQL
   - `scripts/ml/extract_highrisk_data.py` - Build high-risk patient dataset

**Deliverables:**
- ML module structure in place
- Training data extraction working
- First model training script (readmission) functional

---

### Phase 2: First Production Model (Weeks 4-7)

**Goal:** Ship 30-day readmission risk model to production

**Tasks:**
1. **Week 4: Feature Engineering**
   - Extract encounter outcomes (readmitted Y/N)
   - Compute 30-40 features per encounter
   - Handle missing data (imputation)
   - Train/test split (80/20), stratify by outcome

2. **Week 5: Model Training & Tuning**
   - Train baseline logistic regression
   - Train XGBoost with hyperparameter tuning (GridSearchCV)
   - Evaluate AUC-ROC, precision/recall, calibration
   - Select final model (likely XGBoost)

3. **Week 6: Model Validation & Explainability**
   - Cross-validation (5-fold)
   - SHAP values for top features
   - Calibration plot (predicted vs actual risk)
   - Select risk threshold (optimize for 20% alert rate)

4. **Week 7: Integration & Testing**
   - Create `ai/ml/inference/readmission_predictor.py`
   - Add LangGraph tool: `predict_readmission_risk(patient_icn)`
   - Display risk score on patient dashboard (colored badge)
   - Write test scripts, validate predictions

**Deliverables:**
- Readmission risk model in production
- Risk scores visible on dashboard
- LangGraph tool for AI chatbot
- Model documentation (features, performance, thresholds)

---

### Phase 3: Suicide Prevention Risk Model (Weeks 8-11)

**Goal:** Implement REACH VET 2.0 predictive model for suicide risk identification

**Priority: MISSION-CRITICAL** (VA's highest clinical priority)

**Why Second (After Readmission):**
1. **Architectural Alignment:** Both use logistic regression with feature engineeringâ€”same ML patterns
2. **Incremental Complexity:** Adds NLP (NLTK, TextBlob) on top of structured featuresâ€”natural progression
3. **Clinical Priority:** Suicide prevention is VA's #1 priority (REACH VET 2.0 is production VA system)
4. **Design Complete:** 42K-token specification already exists (`docs/spec/suicide-prevention-design.md`)
5. **Reuses Infrastructure:** ML pipeline from Phase 2 + adds NLP capability

**Model Details:**
- **Type:** Binary classification + 3-tier risk stratification (Standard, Moderate, High)
- **Algorithm:** Calibrated logistic regression with published REACH VET 2.0 coefficients
- **Features:** 25-30 features across 4 categories:
  1. **Utilization:** No-show rate, inpatient admits, ED visits
  2. **Medications:** Opioids, benzodiazepines, antidepressants, polypharmacy
  3. **Diagnoses:** Prior suicide attempt, PTSD, TBI, chronic pain, depression, substance use
  4. **Clinical Notes NLP:** Suicidal ideation, hopelessness, social isolation, access to means (firearms)
- **Target Variable:** Suicide-related event within 12 months (binary, from synthetic data initially)
- **Expected Performance:** Sensitivity ~50% at top 0.1% threshold (aligned with published REACH VET 2.0 performance)

**Tasks:**
1. **Week 8: NLP Infrastructure + Feature Engineering**
   - Add dependencies: `nltk>=3.8.1`, `textblob>=0.17.1` to `requirements.txt`
   - Create `ai/ml/features/nlp_features.py` for clinical notes processing
   - Implement keyword extraction for psychosocial risk factors:
     - Suicidal ideation: "suicidal", "SI", "self-harm", "end my life"
     - Hopelessness: "hopeless", "no future", "give up"
     - Social isolation: "alone", "no one", "isolated"
     - Access to means: "gun", "firearm", "pills", "overdose"
   - Implement sentiment analysis using TextBlob
   - Extract structured features (utilization, medications, diagnoses) from PostgreSQL
   - Generate synthetic training data (100+ patients with REACH VET risk patterns)

2. **Week 9: Model Training with Published Coefficients**
   - Implement REACH VET 2.0 logistic regression with literature-based coefficients:
     - Prior suicide attempt: Î² = 2.5
     - PTSD diagnosis: Î² = 1.6
     - MST (Military Sexual Trauma) flag: Î² = 1.8
     - Opioid use: Î² = 1.4
     - No-show rate >20%: Î² = 1.1
     - (15-20 more coefficients from McCarthy et al. 2015)
   - Calculate risk probability: P(suicide) = 1 / (1 + e^-logit)
   - Apply risk tiers: High (>0.01%), Moderate (0.001-0.01%), Standard (<0.001%)
   - Validate on synthetic data (verify high-risk profiles correctly flagged)

3. **Week 10: Explainability + Safety Overrides**
   - Generate explainability factors using coefficient contributions
   - Format: "High risk due to: Prior attempt (40%), PTSD (25%), Opioid use (18%)"
   - Implement safety override: If "suicidal ideation" detected in notes <30 days â†’ Force High tier
   - Store risk scores in `clinical.clinical_risk_scores` table
   - Store contributing factors in `clinical.clinical_risk_factors` table

4. **Week 11: AI Chatbot Integration + Access Controls**
   - Create LangGraph tool: `check_suicide_risk_factors(patient_icn)`
   - Tool accessible ONLY via AI Insights chatbot (no dashboard widgets per ADR-SP-001)
   - Add system prompts with safety guardrails:
     - Never recommend specific methods or means
     - Always include Veterans Crisis Line: 988 then press 1
     - Emphasize clinical judgment supersedes algorithmic assessment
   - Add suggested questions: "What are this patient's suicide risk factors?"
   - Test with synthetic patients (verify explainability and safety overrides)

**Deliverables:**
- Suicide risk model in production (REACH VET 2.0 calibrated)
- NLP pipeline for clinical notes processing (NLTK + TextBlob)
- LangGraph tool for AI chatbot (chatbot-only access)
- Risk scores + explainability factors in PostgreSQL
- Safety override for documented suicidal ideation
- Comprehensive testing with 100+ synthetic patients

**Key Differentiators from Readmission Model:**
- **NLP Addition:** First model to use unstructured clinical notes text
- **Safety-Critical:** Suicide prevention requires stricter safety guardrails
- **Published Coefficients:** Uses literature-validated weights (not data-driven training)
- **Access Pattern:** Chatbot-only (no dashboard visibility to reduce stigma)

---

### Phase 4: High-Risk Patient Model (Weeks 12-15)

**Goal:** Implement composite risk score for resource prioritization

**Tasks:**
1. **Week 12-13: Multi-Output Feature Engineering**
   - ED visit outcome labels
   - Hospital admission outcome labels
   - Cost prediction targets (log-transformed)
   - 40-50 features per patient

2. **Week 14: Model Training**
   - Train 3 separate XGBoost models (ED, admit, cost)
   - Tune hyperparameters for each
   - Create composite score (weighted average)

3. **Week 15: Integration**
   - Create `get_highrisk_score(patient_icn)` tool
   - Add risk tier display to patient list (dashboard)
   - Filter/sort patients by risk tier
   - Generate "Top 10 High-Risk Patients" report

**Deliverables:**
- High-risk score model in production
- Risk tiers on patient list
- Care management workflow support

---

### Phase 5: Care Coordination Tools (Weeks 16-19)

**Goal:** Add medication adherence + care gap prediction

**Tasks:**
1. **Week 16-17: Medication Adherence Model**
   - Extract refill data (requires VistA Pharmacy ETL addition)
   - Train adherence prediction model
   - Integrate into medication page ("Adherence Risk: High")

2. **Week 18-19: Care Gap Prediction**
   - Extract screening history (requires procedure data)
   - Train care gap models (colonoscopy, mammogram, A1C)
   - Display care gaps on patient dashboard

**Deliverables:**
- Adherence risk scores on medication page
- Care gap alerts on dashboard
- Preventive care workflow support

---

### Phase 6: Real-Time Risk Alerts (Weeks 20-22)

**Goal:** Clinical deterioration early warning score

**Tasks:**
1. **Week 20-21: Early Warning Score Model**
   - Extract ICU transfer outcomes
   - Train deterioration prediction model
   - Real-time scoring when vitals refreshed from VistA

2. **Week 22: Integration**
   - Display EWS badge on vital signs widget
   - Trigger alerts for EWS >8 (high risk)
   - Notification to care team (future: SMS/email)

**Deliverables:**
- Early warning score operational
- Real-time risk alerts for inpatient monitoring

---

## 6. Technical Architecture

### 6.1 ML Module Structure

```
ai/ml/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ clinical_features.py     # Extract demographics, vitals, labs
â”‚   â”œâ”€â”€ medication_features.py   # Polypharmacy, high-risk meds
â”‚   â”œâ”€â”€ utilization_features.py  # ED visits, admits, readmits
â”‚   â””â”€â”€ temporal_features.py     # Trends, windows, time-series
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ readmission_model.py     # XGBoost readmission trainer
â”‚   â”œâ”€â”€ highrisk_model.py        # Composite risk score trainer
â”‚   â”œâ”€â”€ adherence_model.py       # Medication adherence trainer
â”‚   â””â”€â”€ utils.py                 # Train/test split, cross-validation
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py               # AUC-ROC, precision/recall, calibration
â”‚   â””â”€â”€ explainability.py        # SHAP, feature importance
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ predictor.py             # Load models, make predictions
â”‚   â””â”€â”€ batch_scoring.py         # Score all patients nightly
â””â”€â”€ models/                      # Serialized model artifacts
    â”œâ”€â”€ readmission_v1.pkl
    â”œâ”€â”€ highrisk_v1.pkl
    â””â”€â”€ model_metadata.json
```

---

### 6.2 Model Training Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Data Extraction (scripts/ml/extract_*.py)            â”‚
â”‚    - Query PostgreSQL clinical tables                   â”‚
â”‚    - Build training dataset (CSV or Parquet)            â”‚
â”‚    - Label outcomes (readmitted Y/N, falls, etc.)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Feature Engineering (ai/ml/features/)                â”‚
â”‚    - Compute 30-50 features per patient/encounter       â”‚
â”‚    - Handle missing data (imputation, median fill)      â”‚
â”‚    - Time windows (last 30 days, 6 months, 1 year)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Train/Test Split                                     â”‚
â”‚    - 80/20 split, stratified by outcome                 â”‚
â”‚    - OR time-based split (train on 2023, test on 2024)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Model Training (ai/ml/training/)                     â”‚
â”‚    - Logistic Regression (baseline)                     â”‚
â”‚    - XGBoost (hyperparameter tuning)                    â”‚
â”‚    - 5-fold cross-validation                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Model Evaluation (ai/ml/evaluation/)                 â”‚
â”‚    - AUC-ROC curve                                      â”‚
â”‚    - Precision-recall curve                             â”‚
â”‚    - Calibration plot (predicted vs actual)             â”‚
â”‚    - SHAP feature importance                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Model Persistence                                    â”‚
â”‚    - Save model: joblib.dump(model, "readmission.pkl")  â”‚
â”‚    - Version metadata: {version, features, AUC, date}   â”‚
â”‚    - Store in ai/ml/models/                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Inference Integration (ai/ml/inference/)             â”‚
â”‚    - Load model at startup                              â”‚
â”‚    - predict_readmission(patient_icn) â†’ 0.35 (35% risk) â”‚
â”‚    - Batch scoring nightly (all patients)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 6.3 Model Serving Options

**Option A: In-Process Prediction (Recommended for Phase 1)**
```python
# ai/ml/inference/predictor.py
import joblib
from ai.ml.features.clinical_features import build_patient_features

class ReadmissionPredictor:
    def __init__(self):
        self.model = joblib.load("ai/ml/models/readmission_v1.pkl")

    def predict(self, patient_icn: str) -> dict:
        # Extract features from PostgreSQL
        features = build_patient_features(patient_icn)

        # Predict risk probability
        risk_prob = self.model.predict_proba(features)[0, 1]

        # Assign risk tier
        if risk_prob >= 0.5:
            tier = "HIGH"
        elif risk_prob >= 0.3:
            tier = "MODERATE"
        else:
            tier = "LOW"

        return {
            "risk_probability": risk_prob,
            "risk_tier": tier,
            "risk_percent": int(risk_prob * 100)
        }
```

**Option B: Batch Pre-Scoring (Nightly Job)**
- Pre-compute risk scores for all patients nightly
- Store in new table: `clinical.patient_risk_scores`
- Fast dashboard queries (no real-time prediction latency)

**Recommendation:** Start with Option A (on-demand), add Option B later for scale.

---

## 7. Model Development Guidelines

### 7.1 Feature Engineering Best Practices

**Time Windows:**
- **Short-term:** Last 7 days, 30 days (acute changes)
- **Medium-term:** Last 6 months (recent patterns)
- **Long-term:** Last 1-3 years (chronic conditions)

**Derived Features:**
```python
# Examples of high-value derived features
- "bp_change_30d": Systolic BP change over 30 days (mmHg)
- "weight_loss_percent_90d": Weight loss % over 90 days
- "ed_visits_6m": Count of ED visits in last 6 months
- "polypharmacy": Binary flag for â‰¥10 active medications
- "high_risk_med_count": Count of fall-risk medications
- "flag_cat1_count": Count of Category I patient flags
- "lab_creatinine_abnormal": Creatinine >1.5 mg/dL
- "days_since_last_admit": Days since most recent admission
```

**Handling Missing Data:**
- Median imputation for continuous (vitals, labs)
- Mode imputation for categorical (sex, facility)
- Indicator variables for "missingness" (e.g., `no_labs_6m`)

---

### 7.2 Model Selection Guidance

| Model Type | Strengths | Weaknesses | Use Cases |
|------------|-----------|------------|-----------|
| **Logistic Regression** | Interpretable, fast, regulatory-friendly | Linear only, limited interactions | Baseline, regulated contexts (EWS) |
| **Random Forest** | Handles non-linearity, feature importance | Overfits on small data | Medium-size datasets (500-5000) |
| **XGBoost** | **Best performance** on tabular data, handles imbalance | Less interpretable, slower training | Readmission, high-risk, all production models |
| **Gradient Boosting** | Good performance, stable | Slower than XGBoost | Alternative to XGBoost if speed OK |

**Recommendation for med-z1:**
- **Baseline:** Logistic Regression (always train first for comparison)
- **Production:** XGBoost (use for all final models)
- **Explainability:** SHAP values for XGBoost (makes it interpretable)

---

### 7.3 Evaluation Metrics

**Primary Metric: AUC-ROC**
- Target: â‰¥0.75 for good clinical utility
- Measures overall discrimination (separates high/low risk)

**Secondary Metrics:**
- **Calibration:** Predicted risk should match actual incidence (calibration plot)
- **Precision @ 20% Recall:** If alerting on top 20%, how accurate are alerts?
- **Sensitivity @ 90% Specificity:** For early warning scores

**Avoid:**
- Accuracy alone (misleading with imbalanced outcomes)
- F1 score alone (doesn't capture risk calibration)

---

### 7.4 Handling Class Imbalance

**Problem:** Readmissions, falls, deterioration are rare events (5-15% incidence)

**Solutions:**
1. **SMOTE (Synthetic Minority Oversampling):**
   ```python
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
   ```

2. **Class Weights:**
   ```python
   xgb_model = XGBClassifier(scale_pos_weight=5)  # Upweight minority class
   ```

3. **Stratified Sampling:**
   ```python
   from sklearn.model_selection import train_test_split
   X_train, X_test, y_train, y_test = train_test_split(
       X, y, test_size=0.2, stratify=y
   )
   ```

---

### 7.5 Model Explainability

**SHAP (SHapley Additive exPlanations):**
```python
import shap

# Explain global model behavior
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Top 10 most important features
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Explain individual prediction
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
```

**Use Case:**
- Clinicians see: "High readmission risk due to: 3 ED visits (6m), polypharmacy (12 meds), recent weight loss"
- Builds trust in ML predictions

---

## 8. Success Metrics

### 8.1 Model Performance Metrics

| Model | Target AUC-ROC | Target Precision @ 20% | Deployment Timeline |
|-------|----------------|------------------------|---------------------|
| 30-Day Readmission | â‰¥0.75 | â‰¥40% | **Phase 2, Week 7** |
| **Suicide Prevention (REACH VET 2.0)** | **N/A** | **Sensitivity â‰¥50% @ 0.1% threshold** | **Phase 3, Week 11** |
| High-Risk Patient Score | â‰¥0.78 | â‰¥50% | Phase 4, Week 15 |
| Medication Adherence | â‰¥0.70 | â‰¥55% | Phase 5, Week 17 |
| Care Gap Prediction | â‰¥0.72 | â‰¥60% | Phase 5, Week 19 |
| Clinical Deterioration (Early Warning Score) | â‰¥0.80 | â‰¥35% | Phase 6, Week 22 |

**Note:** Suicide prevention model uses published REACH VET 2.0 coefficients (not data-driven training), so AUC-ROC from validation studies applies. Primary metric is sensitivity at top 0.1% risk tier (identifies ~50% of future events).

---

### 8.2 Clinical Impact Metrics (6-12 Months Post-Deployment)

**Readmission Reduction:**
- Target: 10-15% reduction in 30-day readmissions
- Measure: Compare readmission rate pre/post ML deployment

**Suicide Prevention:**
- Target: Identify and intervene with top 0.1% risk tier veterans (REACH VET 2.0 approach)
- Measure: Timely safety planning and care coordination for flagged patients
- Impact: Early identification enables proactive mental health intervention

**High-Risk Patient Management:**
- Target: Enroll top 5% risk tier in care coordination (100+ patients)
- Measure: ED visit reduction for enrolled patients

**Care Gap Closure:**
- Target: 20% increase in preventive screening completion
- Measure: Mammogram, colonoscopy, A1C completion rates

---

### 8.3 Adoption Metrics

**Clinician Engagement:**
- Target: 80% of clinicians view risk scores weekly
- Measure: Dashboard analytics (risk badge clicks)

**Alert Action Rate:**
- Target: 50% of high-risk alerts result in action (note, order, referral)
- Measure: EHR audit logs (note documentation after alert)

---

## 9. Risk Mitigation

### 9.1 Clinical Safety Risks

**Risk 1: Over-Reliance on ML Predictions**
- Mitigation: Always display disclaimer: "ML prediction is a clinical decision support tool, not a substitute for clinical judgment"
- Mitigation: Show SHAP feature contributions so clinicians understand "why"

**Risk 2: Alarm Fatigue**
- Mitigation: Set risk thresholds to alert on top 10-20% (not 50%)
- Mitigation: Allow clinicians to dismiss/snooze alerts

**Risk 3: Model Drift**
- Mitigation: Retrain models quarterly on fresh data
- Mitigation: Monitor AUC-ROC on recent data (alert if drops >5%)

---

### 9.2 Regulatory Risks

**Risk: FDA SaMD (Software as Medical Device) Regulation**
- Mitigation: Position models as **clinical decision support**, not diagnostic devices
- Mitigation: Do not claim "prevents readmissions" (claim "identifies high-risk patients")
- Mitigation: Follow FDA guidance for CDS (exempt if clinician-in-the-loop)

**Risk: HIPAA/Privacy Violations**
- Mitigation: All training data is de-identified (ICN is pseudonymous)
- Mitigation: No PHI in model artifacts (only aggregate statistics)
- Mitigation: Model training logs stored in secure environment

---

### 9.3 Bias and Fairness Risks

**Risk: Disparate Impact by Race/Ethnicity**
- Mitigation: Do NOT use race as a feature (risk of encoding bias)
- Mitigation: Evaluate model performance stratified by demographics
- Mitigation: Use fairness metrics (equalized odds) to detect bias

**Example Check:**
```python
from sklearn.metrics import confusion_matrix

# Evaluate for racial/ethnic subgroups
for group in ['White', 'Black', 'Hispanic', 'Other']:
    y_true_group = y_test[demographics['race'] == group]
    y_pred_group = predictions[demographics['race'] == group]
    cm = confusion_matrix(y_true_group, y_pred_group)
    print(f"{group}: Sensitivity={sensitivity}, Specificity={specificity}")
```

---

## 10. Next Steps and Recommendations

### Immediate Actions (Week 1)

1. **Stakeholder Alignment:**
   - Present this assessment to clinical leadership
   - Get buy-in for ML roadmap (Phases 1-5, 18 weeks)
   - Identify clinical champion for model validation

2. **Data Audit:**
   - Verify PostgreSQL has sufficient historical data (â‰¥500 encounters)
   - Identify missing data elements (procedure history, ICU transfers)
   - Plan ETL enhancements for missing data

3. **Resource Allocation:**
   - Assign 1 ML engineer (full-time, 18 weeks)
   - Allocate 0.5 clinical informaticist (model validation, feature definition)
   - Budget for cloud compute (model training ~$500/month)

---

### Key Decisions Needed

**Decision 1: Model Deployment Strategy**
- Option A: Start with readmission model only (prove value, then expand)
- Option B: Build 3 models in parallel (readmission, high-risk, care gap)
- **Recommendation:** Option A (de-risk, learn, iterate)

**Decision 2: Model Governance**
- Who approves model deployment? (Clinical informatics committee?)
- How often to retrain models? (Quarterly recommended)
- Who monitors model performance? (Data science team + clinicians)

**Decision 3: Integration Depth**
- Option A: Display risk scores only (passive decision support)
- Option B: Trigger automated workflows (care management referrals)
- **Recommendation:** Start with Option A, add Option B in Phase 6

---

### Success Criteria for Go/No-Go at Phase 2

After first readmission model is trained (Week 7):

**Go Criteria:**
- AUC-ROC â‰¥0.70 (acceptable discrimination)
- Calibration within 5% (predicted vs actual risk)
- Top 20% risk tier has â‰¥30% readmission rate (concentration of risk)
- Clinical champion validates feature definitions

**No-Go Criteria:**
- AUC-ROC <0.65 (random forest performs no better than baseline)
- Poor calibration (predicted 30% but actual 10%)
- Insufficient training data (<300 encounters with outcomes)

If no-go: Re-evaluate data quality, feature engineering, or target definition before proceeding.

---

## Conclusion

med-z1 has an **excellent foundation** for adding predictive ML capabilities:
- Strong LLM-based conversational AI (best-in-class)
- Comprehensive PostgreSQL clinical data (12 domains)
- Real-time VistA integration (4 domains operational)
- MCP servers for external AI integration

**The gap:** No predictive modeling despite having the right data and infrastructure.

**The opportunity:** Implement 6 high-value ML models that will:
- Reduce 30-day readmissions by 10-15%
- Identify top 5% high-risk patients for proactive care management
- Close care gaps and improve quality metrics
- Predict medication non-adherence and falls

**Recommended approach:** Start with **30-day readmission risk model** (Phase 2, Weeks 4-7), followed immediately by **suicide prevention risk model** (Phase 3, Weeks 8-11). These two models together establish the core ML infrastructure and demonstrate both structured + NLP predictive capabilities. Success here will build momentum for expanding to high-risk scoring, care gaps, and early warning scores.

**Timeline:** 22 weeks to deploy 6 production ML models across 6 phases:
- **Phase 2 (Week 7):** Readmission risk model live
- **Phase 3 (Week 11):** Suicide prevention model live (REACH VET 2.0)
- **Phase 4 (Week 15):** High-risk patient scoring live
- **Phase 5 (Week 19):** Medication adherence + care gap models live
- **Phase 6 (Week 22):** Early warning score (clinical deterioration) live

**Estimated Impact:**
- **Readmission reduction:** 10-15% = $500K-$750K annual savings per facility
- **Suicide prevention:** Early identification of high-risk veterans for timely intervention (mission-critical VA priority)
- **High-risk management:** Proactive care coordination for top 5% resource-intensive patients
- **Care gaps:** 20% increase in preventive screening completion (quality metrics improvement)

---

## References and Resources

### Clinical ML Benchmarks
- **HOSPITAL Score:** 30-day readmission (AUC 0.72), 9 variables
- **LACE Index:** Readmission risk (AUC 0.68), 4 variables
- **NEWS2:** Early warning score (AUC 0.77), vital signs
- **Rothman Index:** Deterioration prediction (AUC 0.80), 43 variables

### Implementation Guides
- Scikit-learn documentation: https://scikit-learn.org/stable/
- XGBoost documentation: https://xgboost.readthedocs.io/
- SHAP explainability: https://shap.readthedocs.io/
- Imbalanced-learn: https://imbalanced-learn.org/

### Regulatory Guidance
- FDA CDS guidance (2022): https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software
- HIPAA de-identification: https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/

---

**Document Prepared By:** Claude Code
**Date:** 2026-02-05
**Version:** 1.0
**Next Review:** After Phase 2 completion (Week 7)
