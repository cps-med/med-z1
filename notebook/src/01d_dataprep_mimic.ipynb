{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - MIMIC-IV Demo Dataset (Community Care)\n",
    "\n",
    "This notebook reads MIMIC-IV Demo CSV files from MinIO and converts them to Parquet format for community care integration.\n",
    "\n",
    "**Purpose**: Prepare PhysioNet MIMIC-IV Demo data for Option C (Concurrent Care) integration  \n",
    "**Source**: `med-sandbox/mimic-data/hosp/*.csv`  \n",
    "**Destination**: `med-data/v1_raw/mimic/*.parquet`\n",
    "\n",
    "## MIMIC-IV Tables\n",
    "\n",
    "- **prescriptions.csv** - Medication orders (analogous to RxOut)\n",
    "- **pharmacy.csv** - Pharmacy dispensing records\n",
    "- **emar.csv** - Medication administrations (analogous to BCMA)\n",
    "- **patients.csv** - Patient demographics\n",
    "- **admissions.csv** - Hospital admissions\n",
    "\n",
    "**Note**: emar_detail.csv was corrupt and is excluded from this integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from dotenv import load_dotenv\n",
    "from importlib.metadata import version\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that dependencies are available for use\n",
    "\n",
    "def print_version():\n",
    "    \"\"\"Display versions of key dependencies\"\"\"\n",
    "    print(\"boto3:\", boto3.__version__)\n",
    "    print(\"pandas:\", pd.__version__)\n",
    "    print(\"s3fs:\", s3fs.__version__)\n",
    "    print(\"pyarrow:\", pa.__version__)\n",
    "    print(\"dotenv:\", version(\"python-dotenv\"))\n",
    "\n",
    "print_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "\n",
    "# Clear any existing handlers to avoid duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configure logging with timestamp and level\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "# Test logging\n",
    "logging.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config module\n",
    "\n",
    "logging.info(f\"Configuration loaded: MinIO endpoint={MINIO_ENDPOINT}\")\n",
    "logging.info(f\"Source: s3://{SOURCE_BUCKET}/{SOURCE_MIMIC_PATH}\")\n",
    "logging.info(f\"Destination: s3://{DEST_BUCKET}/{V1_RAW_MIMIC_PREFIX}\")\n",
    "logging.info(f\"Community care Sta3n: {COMMUNITY_CARE_STA3N}\")\n",
    "logging.info(f\"Community care source system: {COMMUNITY_CARE_SOURCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client for MinIO\n",
    "\n",
    "def create_s3_client():\n",
    "    \"\"\"\n",
    "    Factory function to create S3 client for MinIO (local development).\n",
    "    Returns boto3 S3 client configured for MinIO backend.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Creating MinIO S3 client at {MINIO_ENDPOINT}\")\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=f\"http://{MINIO_ENDPOINT}\",\n",
    "        aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "        aws_secret_access_key=MINIO_SECRET_KEY,\n",
    "        region_name='us-east-1'  # Required but not used by MinIO\n",
    "    )\n",
    "\n",
    "# Create the S3 client\n",
    "s3 = create_s3_client()\n",
    "logging.info(f\"S3 client created successfully\")\n",
    "logging.info(f\"Client type: {type(s3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3FileSystem for pandas/pyarrow I/O\n",
    "\n",
    "logging.info(f\"Initializing S3FileSystem for MinIO at {MINIO_ENDPOINT}\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    key=MINIO_ACCESS_KEY,\n",
    "    secret=MINIO_SECRET_KEY,\n",
    "    client_kwargs={\n",
    "        'endpoint_url': f\"http://{MINIO_ENDPOINT}\"\n",
    "    }\n",
    ")\n",
    "logging.info(\"S3FileSystem created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MIMIC-IV CSV Files\n",
    "\n",
    "Load all MIMIC-IV Demo CSV files from med-sandbox bucket.  \n",
    "**Note**: emar_detail.csv was corrupt and is excluded from this integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MIMIC files to process\n",
    "# Note: emar_detail.csv was corrupt and is excluded\n",
    "\n",
    "mimic_files = {\n",
    "    'prescriptions': 'prescriptions.csv',\n",
    "    'pharmacy': 'pharmacy.csv',\n",
    "    'emar': 'emar.csv',\n",
    "    'patients': 'patients.csv',\n",
    "    'admissions': 'admissions.csv'\n",
    "}\n",
    "\n",
    "logging.info(f\"Processing {len(mimic_files)} MIMIC-IV Demo files\")\n",
    "for table_name, filename in mimic_files.items():\n",
    "    logging.info(f\"  - {table_name}: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each MIMIC CSV file into a DataFrame\n",
    "\n",
    "mimic_data = {}  # Dictionary to store DataFrames\n",
    "total_start_time = time.time()\n",
    "\n",
    "for table_name, filename in mimic_files.items():\n",
    "    csv_uri = f\"s3://{SOURCE_BUCKET}/{SOURCE_MIMIC_PATH}{filename}\"\n",
    "    logging.info(f\"Reading {table_name} from {csv_uri}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read CSV using s3fs storage_options\n",
    "    mimic_data[table_name] = pd.read_csv(\n",
    "        csv_uri,\n",
    "        storage_options={\n",
    "            'key': MINIO_ACCESS_KEY,\n",
    "            'secret': MINIO_SECRET_KEY,\n",
    "            'client_kwargs': {'endpoint_url': f\"http://{MINIO_ENDPOINT}\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    rows = len(mimic_data[table_name])\n",
    "    cols = len(mimic_data[table_name].columns)\n",
    "    logging.info(f\"  ‚úì Loaded {rows:,} rows, {cols} columns in {elapsed:.2f}s\")\n",
    "\n",
    "total_elapsed = time.time() - total_start_time\n",
    "logging.info(f\"All {len(mimic_files)} files loaded in {total_elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Summary\n",
    "\n",
    "Display overview of loaded MIMIC-IV Demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MIMIC-IV Demo dataset summary\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MIMIC-IV DEMO DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for table_name, df in mimic_data.items():\n",
    "    print(f\"{table_name.upper():15} {len(df):>8,} rows  {len(df.columns):>3} columns\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display key statistics\n",
    "logging.info(f\"\\nKey Statistics:\")\n",
    "logging.info(f\"  Unique patients with prescriptions: {mimic_data['prescriptions']['subject_id'].nunique()}\")\n",
    "logging.info(f\"  Unique patients in demographics: {mimic_data['patients']['subject_id'].nunique()}\")\n",
    "logging.info(f\"  Total hospital admissions: {len(mimic_data['admissions']):,}\")\n",
    "\n",
    "# Display date ranges (MIMIC uses shifted dates 2100-2200)\n",
    "if 'starttime' in mimic_data['prescriptions'].columns:\n",
    "    start_date = mimic_data['prescriptions']['starttime'].min()\n",
    "    end_date = mimic_data['prescriptions']['starttime'].max()\n",
    "    logging.info(f\"  Prescription date range: {start_date} to {end_date}\")\n",
    "    logging.info(f\"  Note: MIMIC-IV uses shifted dates (2100-2200) for privacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample prescription data\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PRESCRIPTION DATA (First 5 rows)\")\n",
    "print(\"=\" * 80)\n",
    "display(mimic_data['prescriptions'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample patient demographics\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PATIENT DEMOGRAPHICS (First 5 rows)\")\n",
    "print(\"=\" * 80)\n",
    "display(mimic_data['patients'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Parquet Files to v1_raw/mimic/\n",
    "\n",
    "Convert all MIMIC CSV files to Parquet format and write to med-data bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write each DataFrame to Parquet format\n",
    "\n",
    "logging.info(\"\\nWriting MIMIC data to Parquet format...\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "for table_name, df in mimic_data.items():\n",
    "    # Construct output path\n",
    "    parquet_filename = f\"{table_name}.parquet\"\n",
    "    parquet_uri = f\"s3://{DEST_BUCKET}/{V1_RAW_MIMIC_PREFIX}{parquet_filename}\"\n",
    "    \n",
    "    logging.info(f\"Writing {table_name} to {parquet_uri}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Write to Parquet with compression\n",
    "    df.to_parquet(\n",
    "        parquet_uri,\n",
    "        engine='pyarrow',\n",
    "        filesystem=fs,\n",
    "        compression='snappy',\n",
    "        index=False  # Don't write DataFrame index\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logging.info(f\"  ‚úì Written {len(df):,} rows in {elapsed:.2f}s\")\n",
    "\n",
    "total_elapsed = time.time() - total_start_time\n",
    "logging.info(f\"\\n‚úÖ All {len(mimic_files)} Parquet files written in {total_elapsed:.2f}s\")\n",
    "logging.info(f\"Data location: s3://{DEST_BUCKET}/{V1_RAW_MIMIC_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Parquet Writes\n",
    "\n",
    "Read back each Parquet file to verify successful write and data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify each Parquet file by reading back\n",
    "\n",
    "logging.info(\"\\nVerifying Parquet files...\")\n",
    "verification_results = []\n",
    "\n",
    "for table_name, original_df in mimic_data.items():\n",
    "    parquet_filename = f\"{table_name}.parquet\"\n",
    "    parquet_uri = f\"s3://{DEST_BUCKET}/{V1_RAW_MIMIC_PREFIX}{parquet_filename}\"\n",
    "    \n",
    "    logging.info(f\"Verifying {table_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read back from Parquet\n",
    "    df_verify = pd.read_parquet(parquet_uri, filesystem=fs)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Verify row count\n",
    "    rows_match = len(df_verify) == len(original_df)\n",
    "    cols_match = len(df_verify.columns) == len(original_df.columns)\n",
    "    \n",
    "    if rows_match and cols_match:\n",
    "        logging.info(f\"  ‚úì Verified: {len(df_verify):,} rows, {len(df_verify.columns)} columns ({elapsed:.2f}s)\")\n",
    "        verification_results.append((table_name, True, len(df_verify), len(df_verify.columns)))\n",
    "    else:\n",
    "        logging.error(f\"  ‚úó Mismatch detected for {table_name}!\")\n",
    "        logging.error(f\"    Original: {len(original_df)} rows, {len(original_df.columns)} cols\")\n",
    "        logging.error(f\"    Verified: {len(df_verify)} rows, {len(df_verify.columns)} cols\")\n",
    "        verification_results.append((table_name, False, len(df_verify), len(df_verify.columns)))\n",
    "\n",
    "# Summary\n",
    "all_verified = all([result[1] for result in verification_results])\n",
    "if all_verified:\n",
    "    logging.info(\"\\n‚úÖ All files verified successfully!\")\n",
    "else:\n",
    "    logging.error(\"\\n‚ùå Verification failed for some files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Size Comparison\n",
    "\n",
    "Compare CSV vs Parquet file sizes to show compression benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CSV vs Parquet file sizes\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FILE SIZE COMPARISON (CSV vs Parquet)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Table':<15} {'CSV (MB)':>10} {'Parquet (MB)':>12} {'Compression':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_csv_size = 0\n",
    "total_parquet_size = 0\n",
    "\n",
    "for table_name, filename in mimic_files.items():\n",
    "    # Get CSV file size\n",
    "    csv_key = f\"{SOURCE_MIMIC_PATH}{filename}\"\n",
    "    csv_response = s3.head_object(Bucket=SOURCE_BUCKET, Key=csv_key)\n",
    "    csv_size_mb = csv_response['ContentLength'] / (1024**2)\n",
    "    total_csv_size += csv_size_mb\n",
    "    \n",
    "    # Get Parquet file size\n",
    "    parquet_key = f\"{V1_RAW_MIMIC_PREFIX}{table_name}.parquet\"\n",
    "    parquet_response = s3.head_object(Bucket=DEST_BUCKET, Key=parquet_key)\n",
    "    parquet_size_mb = parquet_response['ContentLength'] / (1024**2)\n",
    "    total_parquet_size += parquet_size_mb\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = (1 - parquet_size_mb / csv_size_mb) * 100 if csv_size_mb > 0 else 0\n",
    "    \n",
    "    print(f\"{table_name:<15} {csv_size_mb:>10.2f} {parquet_size_mb:>12.2f} {compression_ratio:>11.1f}%\")\n",
    "\n",
    "# Calculate total compression\n",
    "total_compression = (1 - total_parquet_size / total_csv_size) * 100 if total_csv_size > 0 else 0\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TOTAL':<15} {total_csv_size:>10.2f} {total_parquet_size:>12.2f} {total_compression:>11.1f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "logging.info(f\"\\nStorage savings: {total_compression:.1f}% ({total_csv_size:.2f} MB ‚Üí {total_parquet_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preparation complete. MIMIC-IV Demo data is now available in Parquet format for community care integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DATA PREPARATION SUMMARY - MIMIC-IV DEMO\")\nprint(\"=\" * 80)\nprint(f\"Source:       s3://{SOURCE_BUCKET}/{SOURCE_MIMIC_PATH}\")\nprint(f\"Destination:  s3://{DEST_BUCKET}/{V1_RAW_MIMIC_PREFIX}\")\nprint()\nprint(\"Files processed:\")\nfor table_name, df in mimic_data.items():\n    print(f\"  {table_name:<15} {len(df):>8,} rows  {len(df.columns):>3} columns\")\nprint()\nprint(f\"Total CSV size:     {total_csv_size:.2f} MB\")\nprint(f\"Total Parquet size: {total_parquet_size:.2f} MB ({total_compression:.1f}% reduction)\")\nprint()\nprint(\"Status:       ‚úÖ Complete\")\nprint(\"=\" * 80)\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Run 01e_mimic_patient_selection.ipynb to integrate community care with VA medications\")\nprint(\"   2. This will create concurrent care scenarios (Option C)\")\nprint(\"   3. Then re-run notebooks 02-06 for complete analysis with dual-source data\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}